{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MNIST MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "legchvewslOL"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZrdPjscqTVN"
      },
      "source": [
        "cd results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSS0fcY-ji0t"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data as data_utils\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GKtI3jY-YWB"
      },
      "source": [
        "'''\n",
        " Function make sure the dataset in training and test have perfectly balanced class.\n",
        " Returns train data, train labels, test data, test labels.\n",
        " Note training size needs to be divisible to the number of classes\n",
        "'''\n",
        "def balanceclass(train_portion, n_classes, x, y):     \n",
        "  trainidx = np.array([None])\n",
        "  testidx = np.array([None])\n",
        "  for i in range(n_classes):  \n",
        "    idx = np.where(y==i)[0]\n",
        "    num = int(len(idx)*train_portion)  #ensure each class has required percentage of training & test samples\n",
        "    print('check idx for this class', idx)\n",
        "    print('check num of this class for training', num)\n",
        "    np.random.shuffle(idx)      \n",
        "    idxtrain = idx[:num]\n",
        "    idxtest = idx[num:]\n",
        "    trainidx = np.append(trainidx, idxtrain)\n",
        "    testidx = np.append(testidx, idxtest)\n",
        "  trainidx = trainidx[1:].astype(int)\n",
        "  testidx = testidx[1:].astype(int)\n",
        "  np.random.shuffle(trainidx)\n",
        "  np.random.shuffle(testidx)\n",
        "  trainx, trainy, testx, testy = x[trainidx], y[trainidx], x[testidx], y[testidx]\n",
        "  return trainx, trainy, testx, testy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbsYrRz6JShh"
      },
      "source": [
        "def flipy(trainy, portion):\n",
        "  labels = np.arange(np.max(trainy+1))\n",
        "  num = int(len(trainy) * portion)\n",
        "  idx = np.random.choice(len(trainy), size = num, replace = False)\n",
        "  new = np.random.choice(labels, size = num, replace = True) \n",
        "  trainy[idx] = new\n",
        "  return trainy, idx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Hc4J2uk4Kt"
      },
      "source": [
        "import os\n",
        "# set seed for current GPU\n",
        "seed = 20170922\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AInGch3UiQiP"
      },
      "source": [
        "'''\n",
        "functions to apply two-point, three-point, four-point mixup on current data and current loss\n",
        "'''\n",
        "def mixup2p(x, y, alpha):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = len(x)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam * x + (1-lam) * x[index, :]\n",
        "    y_a, y_b= y, y[index] \n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion2p(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def mixup3p(x, y, alpha):\n",
        "    lam = np.random.dirichlet(alpha = np.ones(3)*alpha)\n",
        "    batch_size = len(x)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    index2 = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam[0] * x + lam[1] * x[index, :] + lam[2] * x[index2, :]\n",
        "    y_a, y_b, y_c= y, y[index], y[index2]  \n",
        "    return mixed_x, y_a, y_b, y_c, lam\n",
        "\n",
        "def mixup_criterion3p(criterion, pred, y_a, y_b, y_c, lam):\n",
        "    return lam[0] * criterion(pred, y_a) + lam[1] * criterion(pred, y_b) + lam[2] * criterion(pred, y_c)\n",
        "\n",
        "\n",
        "def mixup4p(x, y, alpha):\n",
        "    lam = np.random.dirichlet(alpha = np.ones(4)*alpha)\n",
        "    batch_size = len(x)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    index2 = torch.randperm(batch_size).cuda()\n",
        "    index3 = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam[0] * x + lam[1] * x[index,:] + lam[2] * x[index2,:] + lam[3] * x[index3,:]\n",
        "    y_a, y_b, y_c, y_d= y, y[index], y[index2], y[index3]  \n",
        "    return mixed_x, y_a, y_b, y_c, y_d, lam\n",
        "\n",
        "def mixup_criterion4p(criterion, pred, y_a, y_b, y_c, y_d, lam):\n",
        "    return lam[0] * criterion(pred, y_a) + lam[1] * criterion(pred, y_b) + lam[2] * criterion(pred, y_c) + lam[3]*criterion(pred, y_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bt0vp3Vh7tX"
      },
      "source": [
        "##################### training process\n",
        "def train2p(epoch):\n",
        "    print(a2)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net2.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets_a, targets_b, lam = mixup2p(inputs, targets, a2)\n",
        "        outputs2 = net2(inputs)\n",
        "        loss2 = mixup_criterion2p(criterion, outputs2, targets_a.long(), targets_b.long(), lam)\n",
        "        train_loss += loss2.item()\n",
        "        _, predicted = torch.max(outputs2.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + (1-lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
        "        optimizer2.zero_grad()\n",
        "        loss2.backward()\n",
        "        optimizer2.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), \n",
        "                     'TrainAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "    return 0\n",
        "\n",
        "def train3p(epoch):\n",
        "    print(a3)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net3.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets_a, targets_b, targets_c, lam = mixup3p(inputs, targets, a3)\n",
        "        optimizer3.zero_grad()\n",
        "        outputs3 = net3(inputs)\n",
        "        loss3 = mixup_criterion3p(criterion, outputs3, targets_a.long(), targets_b.long(), targets_c.long(), lam)\n",
        "        train_loss += loss3.item()\n",
        "        _, predicted = torch.max(outputs3.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam[0] * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + lam[1] * predicted.eq(targets_b.data).cpu().sum().float() + lam[2] * predicted.eq(targets_c.data).cpu().sum().float())\n",
        "        loss3.backward()\n",
        "        optimizer3.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), 'TrainAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "def train4p(epoch):\n",
        "    print(a4)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net4.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets_a, targets_b, targets_c, targets_d, lam = mixup4p(inputs, targets, a4)\n",
        "        optimizer4.zero_grad()\n",
        "        outputs4 = net4(inputs)\n",
        "        loss4 = mixup_criterion4p(criterion, outputs4, targets_a.long(), targets_b.long(), targets_c.long(), targets_d.long(), lam)\n",
        "        train_loss += loss4.item()\n",
        "        _, predicted = torch.max(outputs4.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam[0] * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + lam[1] * predicted.eq(targets_b.data).cpu().sum().float() + lam[2] * predicted.eq(targets_c.data).cpu().sum().float() + lam[3]*predicted.eq(targets_d.data).cpu().sum().float())\n",
        "        loss4.backward()\n",
        "        optimizer4.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), 'TrainAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "    return 0\n",
        "\n",
        "def trainori(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    netori.train()\n",
        "    correct, total, train_loss = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizerori.zero_grad()\n",
        "        outputsori = netori(inputs)\n",
        "        lossori = criterion(outputsori, targets.long())      #this returns avg. loss\n",
        "        train_loss += lossori.item()\n",
        "        _, predicted = torch.max(outputsori.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum().float()\n",
        "        lossori.backward()\n",
        "        optimizerori.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), 'TrainAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "    return 0  \n",
        "\n",
        "\n",
        "def test2p(epoch):\n",
        "    global best_acc2\n",
        "    global best_epoch2\n",
        "    net2.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = net2(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "                        \n",
        "    acc = 100.*correct.item()/total\n",
        "    acc2p.append(acc)\n",
        "    if acc > best_acc2:\n",
        "        best_acc2 = acc\n",
        "        best_epoch2 = epoch\n",
        "    return 0\n",
        "\n",
        "def test3p(epoch):\n",
        "    global best_acc3\n",
        "    global best_epoch3\n",
        "    net3.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = net3(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "                        \n",
        "    acc = 100.*correct.item()/total\n",
        "    acc3p.append(acc)\n",
        "    if acc > best_acc3:\n",
        "        best_acc3 = acc\n",
        "        best_epoch3 = epoch\n",
        "    return 0 \n",
        "\n",
        "def test4p(epoch):\n",
        "    global best_acc4\n",
        "    global best_epoch4\n",
        "    net4.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = net4(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))                \n",
        "    acc = 100.*correct.item()/total\n",
        "    acc4p.append(acc)\n",
        "    if acc > best_acc4:\n",
        "        best_acc4 = acc\n",
        "        best_epoch4 = epoch\n",
        "    return 0 \n",
        "\n",
        "def testori(epoch):\n",
        "    global best_accori, best_epochori\n",
        "    netori.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = netori(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "                        \n",
        "    acc = 100.*correct.item()/total\n",
        "    accori.append(acc)\n",
        "    if acc > best_accori:\n",
        "        best_accori = acc\n",
        "        best_epochori = epoch\n",
        "    return 0 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkpk3-m_Kh6U"
      },
      "source": [
        "def trainingmixup():\n",
        "  global results2, results3, results4\n",
        "  results2, results3, results4 = [],[],[]\n",
        "  global acc2p, acc3p, acc4p, best_acc2, best_acc3, best_acc4, best_epoch2, best_epoch3, best_epoch4\n",
        "  acc2p, acc3p, acc4p = [], [], []\n",
        "  global a2, a3, a4\n",
        "  global net2, net3, net4, criterion, optimizer2, optimizer3, optimizer4\n",
        "  # performing grid search to see which value of alpha gives highest acc under given dataset.\n",
        "  for a2 in [0.1, 1, 5]:\n",
        "    best_acc2, best_epoch2 = 0, 0\n",
        "    lr = 0.01\n",
        "    net2 = MLP()\n",
        "    net2.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer2 = optim.SGD(net2.parameters(), lr=lr, momentum=0.9)\n",
        "    for epoch in range(epochs):\n",
        "      train2p(epoch)\n",
        "      test2p(epoch)\n",
        "      if epoch > 80:\n",
        "        for g in optimizer2.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "      if epoch > 150:\n",
        "        for g in optimizer2.param_groups:\n",
        "          g['lr'] = 0.0001\n",
        "    results2.append(np.array(['mixup2p: alpha = ', a2, 'best acc = ',best_acc2, 'best epoch = ', best_epoch2]))\n",
        "\n",
        "  for a34 in [0.1, 1, 5]:\n",
        "    best_acc3, best_acc4, best_epoch3, best_epoch4 = 0, 0, 0, 0\n",
        "    print('\\nstart training mixup 3p')\n",
        "    lr = 0.01\n",
        "    a3 = a34\n",
        "    net3 = MLP()\n",
        "    net3.cuda()\n",
        "    optimizer3 = optim.SGD(net3.parameters(), lr=lr, momentum=0.9)\n",
        "    for epoch in range(epochs):\n",
        "      train3p(epoch)\n",
        "      test3p(epoch)\n",
        "      if epoch > 80:\n",
        "        for g in optimizer3.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "      if epoch > 150:\n",
        "        for g in optimizer3.param_groups:\n",
        "          g['lr'] = 0.0001\n",
        "    results3.append(np.array(['mixup3p: alpha = ', a3, 'best acc = ', best_acc3, 'best epoch = ', best_epoch3]))    #for this value of alpha, the best acc and epoch obtained among 70 epochs given current dataset.\n",
        " \n",
        "    print('\\nstart training mixup 4p')\n",
        "    lr = 0.01\n",
        "    a4 = a34\n",
        "    net4 = MLP()\n",
        "    net4.cuda()\n",
        "    optimizer4 = optim.SGD(net4.parameters(), lr=lr, momentum=0.9)\n",
        "    for epoch in range(epochs):\n",
        "      train4p(epoch)\n",
        "      test4p(epoch)\n",
        "      if epoch > 80:\n",
        "        for g in optimizer4.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "      if epoch > 150:\n",
        "        for g in optimizer4.param_groups:\n",
        "          g['lr'] = 0.0001\n",
        "    results4.append(np.array(['mixup4p: alpha = ', a4, 'best acc = ', best_acc4, 'best epoch = ', best_epoch4]))\n",
        "  return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unqtz65_Kgc5"
      },
      "source": [
        "def trainingERM():\n",
        "  global resultsori, accori, netori, optimizerori, best_accori, best_epochori\n",
        "  resultsori, accori = [],[]\n",
        "  print('\\nStart training ERM')    #train ERM under given dataset\n",
        "  best_accori, best_epochori = 0, 0\n",
        "\n",
        "  lr = 0.01\n",
        "  netori = MLP()\n",
        "  netori.cuda()\n",
        "  optimizerori = optim.SGD(netori.parameters(), lr=lr, momentum=0.9)\n",
        "  for epoch in range(epochs):\n",
        "    trainori(epoch)\n",
        "    testori(epoch)\n",
        "    if epoch > 80:\n",
        "      for g in optimizerori.param_groups:\n",
        "        g['lr'] = 0.001\n",
        "    if epoch > 150:\n",
        "      for g in optimizerori.param_groups:\n",
        "        g['lr'] = 0.0001\n",
        "  resultsori.append(np.array(['ERM: alpha = ', None, 'best acc = ', best_accori, 'best epoch = ', best_epochori]))\n",
        "  return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRuq2yzGpwZu"
      },
      "source": [
        "#################### start training \n",
        "\n",
        "import torchvision\n",
        "train = torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "test = torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "trainx = []\n",
        "for i in range(len(train)):\n",
        "  trainx.append(train[i][0].flatten().numpy())\n",
        "\n",
        "trainx = np.array(trainx)\n",
        "\n",
        "trainlabel = []\n",
        "for i in range(len(train)):\n",
        "  trainlabel.append(train[i][1])\n",
        "\n",
        "trainlabel = np.array(trainlabel)\n",
        "\n",
        "testx = []\n",
        "for i in range(len(test)):\n",
        "  testx.append(test[i][0].flatten().numpy())\n",
        "testx = np.array(testx)\n",
        "\n",
        "testlabel = []\n",
        "for i in range(len(test)):\n",
        "  testlabel.append(test[i][1])\n",
        "testlabel = np.array(testlabel)\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(MLP,self).__init__()\n",
        "      self.fc1 = torch.nn.Linear(len(trainx[0]),800)\n",
        "      self.fc2 = torch.nn.Linear(800,10)\n",
        "        \n",
        "  def forward(self,din):\n",
        "      dout = torch.nn.functional.relu(self.fc1(din))\n",
        "      output = self.fc2(dout)\n",
        "      return output  \n",
        "\n",
        "\n",
        "epochs = 200 \n",
        "result_dict = defaultdict(list)\n",
        "    \n",
        "for run in range(1):    #run one time only\n",
        "  f = open('MNIST all data (EPOCH200).txt', 'a')\n",
        "  train = data_utils.TensorDataset(torch.from_numpy(trainx).float(), torch.from_numpy(trainlabel).float())\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)\n",
        "  test = data_utils.TensorDataset(torch.from_numpy(testx).float(), torch.from_numpy(testlabel).float())\n",
        "  test_loader = data_utils.DataLoader(test, batch_size=128, shuffle=False)\n",
        "      \n",
        "  trainingmixup()     \n",
        "  trainingERM()\n",
        "\n",
        "  #start saving result:\n",
        "  bestalphas = defaultdict(list)    #only need last run's plot\n",
        "  for name,res in zip(['ERM', '2p', '3p', '4p'], [resultsori, results2, results3, results4]):\n",
        "    res = np.array([res])[0]\n",
        "    a = res[:,3].astype(float)\n",
        "    globalbest_acc = np.max(a)    #among all alpha values\n",
        "    idx = np.where(a == np.max(a))[0]\n",
        "    best_alpha = res[:,1].astype(float)[idx]\n",
        "    bestalphas[name].append(best_alpha)\n",
        "    best_epoch = res[:,5].astype(int)[idx]\n",
        "    print(f'for mixup {name}: global best acc = ', globalbest_acc, 'best alpha = ', best_alpha, 'global best epoch = ', best_epoch)\n",
        "    result_dict[name].append(globalbest_acc)\n",
        "    f.write('<{0}  {1}  {2}  {3}  {4} > \\n'.format(run, name, globalbest_acc, best_alpha, best_epoch))\n",
        "\n",
        "f.close()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.title('MNIST')\n",
        "plt.plot(np.arange(len(accori)), accori, label = 'ERM acc')\n",
        "ba2p = bestalphas['2p'][0][0]\n",
        "ba3p = bestalphas['3p'][0][0]\n",
        "ba4p = bestalphas['4p'][0][0]\n",
        "print('ba4p', ba4p)\n",
        "loc2p = np.where([0.1, 1, 8, 12, 32] == ba2p)[0][0]\n",
        "loc3p = np.where([0.1, 1, 5, 12, 50] == ba3p)[0][0]  \n",
        "loc4p = np.where([0.1, 1, 5, 12, 50] == ba4p)[0][0] \n",
        "print('loc4p', loc4p)\n",
        "# plot the test acc over epochs obtained on the last run best alpha\n",
        "plt.plot(np.arange(200), acc2p[loc2p*200 : (loc2p+1)*200], label = '2p best alpha={}'.format(ba2p))\n",
        "plt.plot(np.arange(200), acc3p[loc3p*200 : (loc3p+1)*200], label = '3p best alpha={}'.format(ba3p))\n",
        "plt.plot(np.arange(200), acc4p[loc4p*200 : (loc4p+1)*200], label = '4p best alpha={}'.format(ba4p))\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.savefig('MNIST all data.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}