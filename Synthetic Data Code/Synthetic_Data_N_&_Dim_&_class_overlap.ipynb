{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Synthetic Data N & Dim & class_overlap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRGsrzp-BY6J",
        "outputId": "09a720c0-be52-4148-d250-327548c03c06"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs1-ZMhWBZ9H",
        "outputId": "3db3f94f-1136-43ac-ef74-fe70e1f7158c"
      },
      "source": [
        "cd results/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSS0fcY-ji0t"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data as data_utils\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GKtI3jY-YWB"
      },
      "source": [
        "'''\n",
        " Function make sure the dataset in training and test have perfectly balanced class.\n",
        " Returns train data, train labels, test data, test labels.\n",
        " Note training size needs to be divisible to the number of classes\n",
        "'''\n",
        "def balanceclass(train_portion, n_classes, x, y):     \n",
        "  trainidx = np.array([None])\n",
        "  testidx = np.array([None])\n",
        "  for i in range(n_classes):  \n",
        "    idx = np.where(y==i)[0]\n",
        "    num = int(len(idx)*train_portion)  #ensure each class has required percentage of training & test samples\n",
        "    np.random.shuffle(idx)     \n",
        "    idxtrain = idx[:num]\n",
        "    idxtest = idx[num:]\n",
        "    trainidx = np.append(trainidx, idxtrain)\n",
        "    testidx = np.append(testidx, idxtest)\n",
        "  trainidx = trainidx[1:].astype(int)\n",
        "  testidx = testidx[1:].astype(int)\n",
        "  np.random.shuffle(trainidx)\n",
        "  np.random.shuffle(testidx)\n",
        "  trainx, trainy, testx, testy = x[trainidx], y[trainidx], x[testidx], y[testidx]\n",
        "  return trainx, trainy, testx, testy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbsYrRz6JShh"
      },
      "source": [
        "###### Function to introduce noisy labels\n",
        "def flipy(trainy, portion):\n",
        "  labels = np.arange(np.max(trainy+1))\n",
        "  num = int(len(trainy) * portion)\n",
        "  idx = np.random.choice(len(trainy), size = num, replace = False)\n",
        "  new = np.random.choice(labels, size = num, replace = True) \n",
        "  trainy[idx] = new\n",
        "  return trainy, idx\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Hc4J2uk4Kt"
      },
      "source": [
        "import os\n",
        "# set seed for current GPU\n",
        "seed = 20170922\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "cudnn.deterministic = True"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AInGch3UiQiP"
      },
      "source": [
        "'''\n",
        "functions to apply two-point, three-point, four-point mixup on current data and current loss\n",
        "'''\n",
        "def mixup2p(x, y, alpha):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = len(x)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam * x + (1-lam) * x[index, :]\n",
        "    y_a, y_b= y, y[index]  \n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion2p(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)   #mix up losses\n",
        "\n",
        "\n",
        "def mixup3p(x, y, alpha):\n",
        "    lam = np.random.dirichlet(alpha = np.ones(3)*alpha)\n",
        "    batch_size = len(x)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    index2 = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam[0] * x + lam[1] * x[index, :] + lam[2] * x[index2, :]\n",
        "    y_a, y_b, y_c= y, y[index], y[index2] \n",
        "    return mixed_x, y_a, y_b, y_c, lam\n",
        "\n",
        "def mixup_criterion3p(criterion, pred, y_a, y_b, y_c, lam):\n",
        "    return lam[0] * criterion(pred, y_a) + lam[1] * criterion(pred, y_b) + lam[2] * criterion(pred, y_c)\n",
        "\n",
        "\n",
        "def mixup4p(x, y, alpha):\n",
        "    lam = np.random.dirichlet(alpha = np.ones(4)*alpha)\n",
        "    batch_size = len(x)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "    index2 = torch.randperm(batch_size).cuda()\n",
        "    index3 = torch.randperm(batch_size).cuda()\n",
        "    mixed_x = lam[0] * x + lam[1] * x[index,:] + lam[2] * x[index2,:] + lam[3] * x[index3,:]\n",
        "    y_a, y_b, y_c, y_d= y, y[index], y[index2], y[index3] \n",
        "    return mixed_x, y_a, y_b, y_c, y_d, lam\n",
        "\n",
        "def mixup_criterion4p(criterion, pred, y_a, y_b, y_c, y_d, lam):\n",
        "    return lam[0] * criterion(pred, y_a) + lam[1] * criterion(pred, y_b) + lam[2] * criterion(pred, y_c) + lam[3]*criterion(pred, y_d)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaiXZ015fZV-"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bt0vp3Vh7tX"
      },
      "source": [
        "############################## training process ################################\n",
        "def train2p(epoch):\n",
        "    print(a2)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net2.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets_a, targets_b, lam = mixup2p(inputs, targets, a2)\n",
        "        outputs2 = net2(inputs)\n",
        "        loss2 = mixup_criterion2p(criterion, outputs2, targets_a.long(), targets_b.long(), lam)\n",
        "        train_loss += loss2.item()\n",
        "        _, predicted = torch.max(outputs2.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + (1-lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
        "        optimizer2.zero_grad()\n",
        "        loss2.backward()\n",
        "        optimizer2.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), \n",
        "                     'TrainAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "    return 0\n",
        "\n",
        "def train3p(epoch):\n",
        "    print(a3)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net3.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets_a, targets_b, targets_c, lam = mixup3p(inputs, targets, a3)\n",
        "        optimizer3.zero_grad()\n",
        "        outputs3 = net3(inputs)\n",
        "        loss3 = mixup_criterion3p(criterion, outputs3, targets_a.long(), targets_b.long(), targets_c.long(), lam)\n",
        "        train_loss += loss3.item()\n",
        "        _, predicted = torch.max(outputs3.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam[0] * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + lam[1] * predicted.eq(targets_b.data).cpu().sum().float() + lam[2] * predicted.eq(targets_c.data).cpu().sum().float())\n",
        "        loss3.backward()\n",
        "        optimizer3.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), 'TrainAcc: {:.3f}'.format(100.*correct.item()/total))    \n",
        "    return 0\n",
        "\n",
        "\n",
        "def train4p(epoch):\n",
        "    print(a4)\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net4.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets_a, targets_b, targets_c, targets_d, lam = mixup4p(inputs, targets, a4)\n",
        "        optimizer4.zero_grad()\n",
        "        outputs4 = net4(inputs)\n",
        "        loss4 = mixup_criterion4p(criterion, outputs4, targets_a.long(), targets_b.long(), targets_c.long(), targets_d.long(), lam)\n",
        "        train_loss += loss4.item()\n",
        "        _, predicted = torch.max(outputs4.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam[0] * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + lam[1] * predicted.eq(targets_b.data).cpu().sum().float() + lam[2] * predicted.eq(targets_c.data).cpu().sum().float() + lam[3]*predicted.eq(targets_d.data).cpu().sum().float())\n",
        "        loss4.backward()\n",
        "        optimizer4.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), 'TrainAcc: {:.3f}'.format(100.*correct.item()/total))    \n",
        "    return 0\n",
        "\n",
        "def trainori(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    netori.train()\n",
        "    correct, total, train_loss = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizerori.zero_grad()\n",
        "        outputsori = netori(inputs)\n",
        "        lossori = criterion(outputsori, targets.long())      \n",
        "        train_loss += lossori.item()\n",
        "        _, predicted = torch.max(outputsori.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum().float()\n",
        "        lossori.backward()\n",
        "        optimizerori.step()\n",
        "    print('TrainLoss: {:.3f}'.format(train_loss/(batch_idx+1)), 'TrainAcc: {:.3f}'.format(100.*correct.item()/total))   \n",
        "    return 0  \n",
        "\n",
        "\n",
        "def test2p(epoch):\n",
        "    global best_acc2\n",
        "    global best_epoch2\n",
        "    net2.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = net2(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "                        \n",
        "    acc = 100.*correct.item()/total\n",
        "    acc2p.append(acc)\n",
        "    if acc > best_acc2:\n",
        "        best_acc2 = acc\n",
        "        best_epoch2 = epoch\n",
        "    return 0\n",
        "\n",
        "def test3p(epoch):\n",
        "    global best_acc3\n",
        "    global best_epoch3\n",
        "    net3.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = net3(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "                        \n",
        "    acc = 100.*correct.item()/total\n",
        "    acc3p.append(acc)\n",
        "    if acc > best_acc3:\n",
        "        best_acc3 = acc\n",
        "        best_epoch3 = epoch\n",
        "    return 0 \n",
        "\n",
        "def test4p(epoch):\n",
        "    global best_acc4\n",
        "    global best_epoch4\n",
        "    net4.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = net4(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))                \n",
        "    acc = 100.*correct.item()/total\n",
        "    acc4p.append(acc)\n",
        "    if acc > best_acc4:\n",
        "        best_acc4 = acc\n",
        "        best_epoch4 = epoch\n",
        "    return 0 \n",
        "\n",
        "def testori(epoch):\n",
        "    global best_accori, best_epochori\n",
        "    netori.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = netori(inputs)\n",
        "        loss = criterion(outputs, targets.long())\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    print('TestLoss:{:.3f}'.format(test_loss/(batch_idx+1)), 'TestAcc: {:.3f}'.format(100.*correct.item()/total))\n",
        "                        \n",
        "    acc = 100.*correct.item()/total\n",
        "    accori.append(acc)\n",
        "    if acc > best_accori:\n",
        "        best_accori = acc\n",
        "        best_epochori = epoch\n",
        "    return 0 \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkpk3-m_Kh6U"
      },
      "source": [
        "def trainingmixup():\n",
        "  global results2, results3, results4\n",
        "  results2, results3, results4 = [],[],[]\n",
        "  global acc2p, acc3p, acc4p, best_acc2, best_acc3, best_acc4, best_epoch2, best_epoch3, best_epoch4\n",
        "  acc2p, acc3p, acc4p = [], [], []\n",
        "  global a2, a3, a4\n",
        "  global net2, net3, net4, criterion, optimizer2, optimizer3, optimizer4\n",
        "  # performing grid search to see which value sof alpha gives highest acc under given dataset.\n",
        "  for a2 in [0.1, 1, 8, 12, 32]:\n",
        "    best_acc2, best_epoch2 = 0, 0\n",
        "    lr = 0.01\n",
        "    net2 = MLP()\n",
        "    net2.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer2 = optim.SGD(net2.parameters(), lr=lr, momentum=0.9)\n",
        "    for epoch in range(epochsnew):\n",
        "      train2p(epoch)\n",
        "      test2p(epoch)\n",
        "      if epoch > 120:\n",
        "        for g in optimizer2.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "      if epoch > 220:\n",
        "        for g in optimizer2.param_groups:\n",
        "          g['lr'] = 0.0001\n",
        "    results2.append(np.array(['mixup2p: alpha = ', a2, 'best acc = ',best_acc2, 'best epoch = ', best_epoch2])) \n",
        "\n",
        "  for a34 in [0.1, 1, 5, 12, 50]:\n",
        "    best_acc3, best_acc4, best_epoch3, best_epoch4 = 0, 0, 0, 0\n",
        "    print('\\nstart training mixup 3p')\n",
        "    lr = 0.01\n",
        "    a3 = a34\n",
        "    net3 = MLP()\n",
        "    net3.cuda()\n",
        "    optimizer3 = optim.SGD(net3.parameters(), lr=lr, momentum=0.9)\n",
        "    for epoch in range(epochsnew):\n",
        "      train3p(epoch)\n",
        "      test3p(epoch)\n",
        "      if epoch > 120:\n",
        "        for g in optimizer3.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "      if epoch > 220:\n",
        "        for g in optimizer3.param_groups:\n",
        "          g['lr'] = 0.0001\n",
        "    results3.append(np.array(['mixup3p: alpha = ', a3, 'best acc = ', best_acc3, 'best epoch = ', best_epoch3]))    #for this value of alpha, the best acc and epoch obtained among 70 epochs given current dataset.\n",
        "\n",
        "    print('\\nstart training mixup 4p')\n",
        "    lr = 0.01\n",
        "    a4 = a34\n",
        "    net4 = MLP()\n",
        "    net4.cuda()\n",
        "    optimizer4 = optim.SGD(net4.parameters(), lr=lr, momentum=0.9)\n",
        "    for epoch in range(epochsnew):\n",
        "      train4p(epoch)\n",
        "      test4p(epoch)\n",
        "      if epoch > 120:\n",
        "        for g in optimizer4.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "      if epoch > 220:\n",
        "        for g in optimizer4.param_groups:\n",
        "          g['lr'] = 0.0001\n",
        "    results4.append(np.array(['mixup4p: alpha = ', a4, 'best acc = ', best_acc4, 'best epoch = ', best_epoch4]))\n",
        "  return 0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unqtz65_Kgc5"
      },
      "source": [
        "def trainingERM():\n",
        "  global resultsori, accori, netori, optimizerori, best_accori, best_epochori\n",
        "  resultsori, accori = [],[]\n",
        "  print('\\nStart training ERM')    #train ERM under the given dataset\n",
        "  best_accori, best_epochori = 0, 0\n",
        "  \n",
        "  lr = 0.01\n",
        "  netori = MLP()\n",
        "  netori.cuda()\n",
        "  optimizerori = optim.SGD(netori.parameters(), lr=lr, momentum=0.9)\n",
        "  for epoch in range(epochsnew):\n",
        "    trainori(epoch)\n",
        "    testori(epoch)\n",
        "    if epoch > 120:\n",
        "      for g in optimizerori.param_groups:\n",
        "        g['lr'] = 0.001\n",
        "    if epoch > 220:\n",
        "      for g in optimizerori.param_groups:\n",
        "        g['lr'] = 0.0001\n",
        "  resultsori.append(np.array(['ERM: alpha = ', None, 'best acc = ', best_accori, 'best epoch = ', best_epochori]))\n",
        "  return 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cRuq2yzGpwZu",
        "outputId": "6cf80915-5513-4838-a509-6c59d8f2794f"
      },
      "source": [
        "############################## start training ################################\n",
        "for dim in [2000]:\n",
        "  class MLP(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(MLP,self).__init__()\n",
        "          self.fc1 = torch.nn.Linear(dim,800)\n",
        "          self.fc2 = torch.nn.Linear(800,10)\n",
        "        \n",
        "      def forward(self,din):\n",
        "          dout = torch.nn.functional.relu(self.fc1(din))\n",
        "          output = self.fc2(dout)\n",
        "          return output  \n",
        "  for n in [200]:  \n",
        "    epochsnew = 300 \n",
        "    result_dict = defaultdict(list)\n",
        "    f = open('dim={} n={}.txt'.format(dim, n), 'a')\n",
        "    for run in range(10):     #run 10 times and take avg.\n",
        "      x,y = sklearn.datasets.make_classification(n_samples=n, n_features=dim, n_informative=dim, n_redundant=0,\n",
        "\t\t\t\t\tn_repeated=0, n_classes=10, n_clusters_per_class=2, weights=None,\n",
        " \t\t\t\t\tflip_y=0, class_sep=1, hypercube=True,shift=0.0, scale=1.0, \n",
        "\t\t\t\t\tshuffle=True, random_state=None)\n",
        "\n",
        "      train_x, train_y, test_x, test_y = balanceclass(0.75, 10, x, y)\n",
        "      train = data_utils.TensorDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float())\n",
        "      train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)\n",
        "      test = data_utils.TensorDataset(torch.from_numpy(test_x).float(), torch.from_numpy(test_y).float())\n",
        "      test_loader = data_utils.DataLoader(test, batch_size=128, shuffle=False)\n",
        "      \n",
        "      trainingmixup()\n",
        "      trainingERM()\n",
        "      #start saving every run's result:\n",
        "      bestalphas = defaultdict(list)    #only need last run's plot\n",
        "      for name,res in zip(['ERM', '2p', '3p', '4p'], [resultsori, results2, results3, results4]):\n",
        "        res = np.array([res])[0]\n",
        "        a = res[:,3].astype(float)\n",
        "        globalbest_acc = np.max(a)    #pick the maximum among all alpha values\n",
        "        idx = np.where(a == np.max(a))[0]\n",
        "        best_alpha = res[:,1].astype(float)[idx]\n",
        "        bestalphas[name].append(best_alpha)\n",
        "        best_epoch = res[:,5].astype(int)[idx]\n",
        "        print(f'for mixup {name}: global best acc = ', globalbest_acc, 'best alpha = ', best_alpha, 'global best epoch = ', best_epoch)\n",
        "        result_dict[name].append(globalbest_acc)\n",
        "        f.write('<{0}  {1}  {2}  {3}  {4} > \\n'.format(run, name, globalbest_acc, best_alpha, best_epoch))\n",
        "    #computing mean over 10 runs\n",
        "    for i in ['2p','3p','4p','ERM']:\n",
        "      mean = round(np.mean(result_dict[i]),4)\n",
        "      std = round(np.std(result_dict[i]),4)\n",
        "      f.write('start computing mean and std: \\n')\n",
        "      f.write('<{0}  {1}  {2}> \\n'.format(i, mean, std))\n",
        "    f.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('n={} d={}'.format(n,dim))\n",
        "    plt.plot(np.arange(len(accori)), accori, label = 'ERM acc')\n",
        "    ba2p = bestalphas['2p'][0][0]\n",
        "    ba3p = bestalphas['3p'][0][0]\n",
        "    ba4p = bestalphas['4p'][0][0]\n",
        "    print('ba4p', ba4p)\n",
        "    loc2p = np.where([0.1, 1, 8, 12, 32] == ba2p)[0][0]\n",
        "    loc3p = np.where([0.1, 1, 5, 12, 50] == ba3p)[0][0]  \n",
        "    loc4p = np.where([0.1, 1, 5, 12, 50] == ba4p)[0][0] \n",
        "    print('loc4p', loc4p)\n",
        "# plot the test acc over epochs obtained on the last run best alpha\n",
        "    plt.plot(np.arange(300), acc2p[loc2p*300 : (loc2p+1)*300], label = '2p best alpha={}'.format(ba2p))\n",
        "    plt.plot(np.arange(300), acc3p[loc3p*300 : (loc3p+1)*300], label = '3p best alpha={}'.format(ba3p))\n",
        "    plt.plot(np.arange(300), acc4p[loc4p*300 : (loc4p+1)*300], label = '4p best alpha={}'.format(ba4p))\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('test accuracy')\n",
        "    plt.savefig('n={} d={}.png'.format(n, dim))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "Epoch: 142\n",
            "TrainLoss: 1672479.688 TrainAcc: 31.167\n",
            "TestLoss:8477970.000 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 143\n",
            "TrainLoss: 1384395.438 TrainAcc: 31.812\n",
            "TestLoss:8311671.000 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 144\n",
            "TrainLoss: 1101300.188 TrainAcc: 30.324\n",
            "TestLoss:8141936.000 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 145\n",
            "TrainLoss: 1359232.000 TrainAcc: 28.048\n",
            "TestLoss:7941515.000 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 146\n",
            "TrainLoss: 1180709.688 TrainAcc: 30.529\n",
            "TestLoss:7768722.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 147\n",
            "TrainLoss: 1156538.375 TrainAcc: 28.452\n",
            "TestLoss:7725936.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 148\n",
            "TrainLoss: 1149063.062 TrainAcc: 34.252\n",
            "TestLoss:7741019.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 149\n",
            "TrainLoss: 1266770.438 TrainAcc: 31.922\n",
            "TestLoss:7752361.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 150\n",
            "TrainLoss: 1119261.438 TrainAcc: 31.496\n",
            "TestLoss:7757078.500 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 151\n",
            "TrainLoss: 1147878.438 TrainAcc: 29.255\n",
            "TestLoss:7806690.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 152\n",
            "TrainLoss: 1043967.219 TrainAcc: 29.451\n",
            "TestLoss:7956484.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 153\n",
            "TrainLoss: 998758.031 TrainAcc: 30.796\n",
            "TestLoss:8076360.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 154\n",
            "TrainLoss: 1049815.125 TrainAcc: 41.203\n",
            "TestLoss:8042824.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 155\n",
            "TrainLoss: 998257.156 TrainAcc: 30.838\n",
            "TestLoss:7970280.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 156\n",
            "TrainLoss: 1013335.906 TrainAcc: 33.343\n",
            "TestLoss:7919670.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 157\n",
            "TrainLoss: 875798.156 TrainAcc: 32.280\n",
            "TestLoss:7937038.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 158\n",
            "TrainLoss: 850221.969 TrainAcc: 31.065\n",
            "TestLoss:7924424.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 159\n",
            "TrainLoss: 1028388.062 TrainAcc: 30.954\n",
            "TestLoss:7846071.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 160\n",
            "TrainLoss: 1053828.594 TrainAcc: 31.077\n",
            "TestLoss:7792390.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 161\n",
            "TrainLoss: 983512.375 TrainAcc: 33.260\n",
            "TestLoss:7806468.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 162\n",
            "TrainLoss: 894482.688 TrainAcc: 28.788\n",
            "TestLoss:7734720.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 163\n",
            "TrainLoss: 911215.812 TrainAcc: 31.964\n",
            "TestLoss:7664068.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 164\n",
            "TrainLoss: 822042.594 TrainAcc: 27.682\n",
            "TestLoss:7566404.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 165\n",
            "TrainLoss: 733953.719 TrainAcc: 30.734\n",
            "TestLoss:7483825.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 166\n",
            "TrainLoss: 742123.250 TrainAcc: 28.622\n",
            "TestLoss:7440610.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 167\n",
            "TrainLoss: 693654.781 TrainAcc: 28.679\n",
            "TestLoss:7405517.500 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 168\n",
            "TrainLoss: 718713.656 TrainAcc: 29.111\n",
            "TestLoss:7381208.500 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 169\n",
            "TrainLoss: 800230.875 TrainAcc: 29.280\n",
            "TestLoss:7424544.000 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 170\n",
            "TrainLoss: 830311.812 TrainAcc: 33.865\n",
            "TestLoss:7520311.500 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 171\n",
            "TrainLoss: 750082.156 TrainAcc: 29.240\n",
            "TestLoss:7694990.000 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 172\n",
            "TrainLoss: 715719.469 TrainAcc: 28.569\n",
            "TestLoss:7886994.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 173\n",
            "TrainLoss: 809210.438 TrainAcc: 29.768\n",
            "TestLoss:8017594.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 174\n",
            "TrainLoss: 646235.781 TrainAcc: 36.190\n",
            "TestLoss:8063785.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 175\n",
            "TrainLoss: 805955.094 TrainAcc: 29.942\n",
            "TestLoss:8027740.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 176\n",
            "TrainLoss: 725906.969 TrainAcc: 31.088\n",
            "TestLoss:7872052.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 177\n",
            "TrainLoss: 559039.062 TrainAcc: 27.951\n",
            "TestLoss:7717101.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 178\n",
            "TrainLoss: 651510.438 TrainAcc: 28.869\n",
            "TestLoss:7628742.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 179\n",
            "TrainLoss: 747471.781 TrainAcc: 30.991\n",
            "TestLoss:7626096.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 180\n",
            "TrainLoss: 577354.094 TrainAcc: 31.838\n",
            "TestLoss:7661472.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 181\n",
            "TrainLoss: 588258.969 TrainAcc: 28.226\n",
            "TestLoss:7656138.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 182\n",
            "TrainLoss: 684322.438 TrainAcc: 41.813\n",
            "TestLoss:7620726.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 183\n",
            "TrainLoss: 538671.281 TrainAcc: 30.485\n",
            "TestLoss:7562532.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 184\n",
            "TrainLoss: 474556.719 TrainAcc: 30.146\n",
            "TestLoss:7486043.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 185\n",
            "TrainLoss: 496130.625 TrainAcc: 32.479\n",
            "TestLoss:7475019.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 186\n",
            "TrainLoss: 445084.672 TrainAcc: 32.478\n",
            "TestLoss:7429129.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 187\n",
            "TrainLoss: 621244.688 TrainAcc: 34.172\n",
            "TestLoss:7314244.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 188\n",
            "TrainLoss: 566230.797 TrainAcc: 29.159\n",
            "TestLoss:7280952.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 189\n",
            "TrainLoss: 576226.047 TrainAcc: 32.507\n",
            "TestLoss:7325138.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 190\n",
            "TrainLoss: 522408.109 TrainAcc: 27.926\n",
            "TestLoss:7345744.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 191\n",
            "TrainLoss: 554932.500 TrainAcc: 30.688\n",
            "TestLoss:7295998.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 192\n",
            "TrainLoss: 470534.172 TrainAcc: 29.784\n",
            "TestLoss:7228994.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 193\n",
            "TrainLoss: 422465.031 TrainAcc: 27.040\n",
            "TestLoss:7198585.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 194\n",
            "TrainLoss: 531890.891 TrainAcc: 29.876\n",
            "TestLoss:7110459.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 195\n",
            "TrainLoss: 510054.531 TrainAcc: 28.307\n",
            "TestLoss:6975020.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 196\n",
            "TrainLoss: 462364.156 TrainAcc: 28.358\n",
            "TestLoss:6875216.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 197\n",
            "TrainLoss: 603302.938 TrainAcc: 28.358\n",
            "TestLoss:6897566.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 198\n",
            "TrainLoss: 578290.531 TrainAcc: 34.057\n",
            "TestLoss:7006981.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 199\n",
            "TrainLoss: 526328.703 TrainAcc: 31.067\n",
            "TestLoss:7139699.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 200\n",
            "TrainLoss: 451363.328 TrainAcc: 30.688\n",
            "TestLoss:7277579.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 201\n",
            "TrainLoss: 433029.594 TrainAcc: 31.654\n",
            "TestLoss:7351382.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 202\n",
            "TrainLoss: 382936.875 TrainAcc: 31.793\n",
            "TestLoss:7374932.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 203\n",
            "TrainLoss: 402992.141 TrainAcc: 27.617\n",
            "TestLoss:7338747.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 204\n",
            "TrainLoss: 420446.344 TrainAcc: 26.845\n",
            "TestLoss:7290388.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 205\n",
            "TrainLoss: 412580.672 TrainAcc: 32.314\n",
            "TestLoss:7207391.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 206\n",
            "TrainLoss: 507938.062 TrainAcc: 31.179\n",
            "TestLoss:7043284.500 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 207\n",
            "TrainLoss: 411686.406 TrainAcc: 28.905\n",
            "TestLoss:6866974.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 208\n",
            "TrainLoss: 411404.484 TrainAcc: 30.077\n",
            "TestLoss:6741464.500 TestAcc: 12.000\n",
            "12\n",
            "\n",
            "Epoch: 209\n",
            "TrainLoss: 421687.000 TrainAcc: 32.662\n",
            "TestLoss:6660590.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 210\n",
            "TrainLoss: 350438.188 TrainAcc: 27.536\n",
            "TestLoss:6596718.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 211\n",
            "TrainLoss: 372224.156 TrainAcc: 29.254\n",
            "TestLoss:6541378.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 212\n",
            "TrainLoss: 390285.250 TrainAcc: 29.025\n",
            "TestLoss:6535594.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 213\n",
            "TrainLoss: 397291.938 TrainAcc: 31.912\n",
            "TestLoss:6617257.000 TestAcc: 10.000\n",
            "12\n",
            "\n",
            "Epoch: 214\n",
            "TrainLoss: 336523.875 TrainAcc: 28.865\n",
            "TestLoss:6714763.000 TestAcc: 6.000\n",
            "12\n",
            "\n",
            "Epoch: 215\n",
            "TrainLoss: 291544.844 TrainAcc: 27.597\n",
            "TestLoss:6803888.000 TestAcc: 6.000\n",
            "12\n",
            "\n",
            "Epoch: 216\n",
            "TrainLoss: 375254.016 TrainAcc: 26.464\n",
            "TestLoss:6872052.000 TestAcc: 6.000\n",
            "12\n",
            "\n",
            "Epoch: 217\n",
            "TrainLoss: 358283.078 TrainAcc: 26.651\n",
            "TestLoss:6957577.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 218\n",
            "TrainLoss: 396878.109 TrainAcc: 32.641\n",
            "TestLoss:7018996.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 219\n",
            "TrainLoss: 415264.875 TrainAcc: 28.394\n",
            "TestLoss:7058933.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 220\n",
            "TrainLoss: 280212.172 TrainAcc: 32.020\n",
            "TestLoss:7059229.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 221\n",
            "TrainLoss: 302732.844 TrainAcc: 31.285\n",
            "TestLoss:7053648.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 222\n",
            "TrainLoss: 245264.484 TrainAcc: 28.770\n",
            "TestLoss:7056424.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 223\n",
            "TrainLoss: 397003.125 TrainAcc: 26.206\n",
            "TestLoss:7054996.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 224\n",
            "TrainLoss: 326094.828 TrainAcc: 28.970\n",
            "TestLoss:7046902.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 225\n",
            "TrainLoss: 324774.000 TrainAcc: 33.941\n",
            "TestLoss:7037512.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 226\n",
            "TrainLoss: 317981.359 TrainAcc: 27.567\n",
            "TestLoss:7025389.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 227\n",
            "TrainLoss: 342624.156 TrainAcc: 31.709\n",
            "TestLoss:7014048.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 228\n",
            "TrainLoss: 359389.797 TrainAcc: 28.944\n",
            "TestLoss:6997224.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 229\n",
            "TrainLoss: 398685.250 TrainAcc: 31.235\n",
            "TestLoss:6965796.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 230\n",
            "TrainLoss: 316423.938 TrainAcc: 31.158\n",
            "TestLoss:6924599.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 231\n",
            "TrainLoss: 310468.484 TrainAcc: 29.243\n",
            "TestLoss:6887089.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 232\n",
            "TrainLoss: 300008.781 TrainAcc: 30.744\n",
            "TestLoss:6854480.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 233\n",
            "TrainLoss: 310133.750 TrainAcc: 29.393\n",
            "TestLoss:6832637.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 234\n",
            "TrainLoss: 332594.438 TrainAcc: 31.151\n",
            "TestLoss:6814742.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 235\n",
            "TrainLoss: 355986.609 TrainAcc: 28.730\n",
            "TestLoss:6801595.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 236\n",
            "TrainLoss: 367429.734 TrainAcc: 29.988\n",
            "TestLoss:6793170.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 237\n",
            "TrainLoss: 297078.266 TrainAcc: 30.404\n",
            "TestLoss:6787285.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 238\n",
            "TrainLoss: 329598.484 TrainAcc: 38.411\n",
            "TestLoss:6780087.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 239\n",
            "TrainLoss: 261924.922 TrainAcc: 29.373\n",
            "TestLoss:6775033.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 240\n",
            "TrainLoss: 280352.844 TrainAcc: 31.615\n",
            "TestLoss:6770480.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 241\n",
            "TrainLoss: 242893.852 TrainAcc: 29.431\n",
            "TestLoss:6764272.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 242\n",
            "TrainLoss: 376611.953 TrainAcc: 30.655\n",
            "TestLoss:6760255.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 243\n",
            "TrainLoss: 276097.375 TrainAcc: 30.890\n",
            "TestLoss:6753401.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 244\n",
            "TrainLoss: 297272.195 TrainAcc: 27.995\n",
            "TestLoss:6746374.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 245\n",
            "TrainLoss: 221361.922 TrainAcc: 32.116\n",
            "TestLoss:6742282.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 246\n",
            "TrainLoss: 292708.688 TrainAcc: 32.627\n",
            "TestLoss:6741154.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 247\n",
            "TrainLoss: 232465.031 TrainAcc: 27.606\n",
            "TestLoss:6739842.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 248\n",
            "TrainLoss: 299195.797 TrainAcc: 28.926\n",
            "TestLoss:6741207.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 249\n",
            "TrainLoss: 309143.344 TrainAcc: 29.202\n",
            "TestLoss:6739725.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 250\n",
            "TrainLoss: 304097.328 TrainAcc: 29.684\n",
            "TestLoss:6740086.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 251\n",
            "TrainLoss: 219340.078 TrainAcc: 32.365\n",
            "TestLoss:6740477.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 252\n",
            "TrainLoss: 292160.484 TrainAcc: 31.967\n",
            "TestLoss:6742584.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 253\n",
            "TrainLoss: 270054.094 TrainAcc: 32.203\n",
            "TestLoss:6749719.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 254\n",
            "TrainLoss: 275706.344 TrainAcc: 32.995\n",
            "TestLoss:6755963.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 255\n",
            "TrainLoss: 263250.336 TrainAcc: 30.485\n",
            "TestLoss:6760383.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 256\n",
            "TrainLoss: 255383.094 TrainAcc: 31.016\n",
            "TestLoss:6764777.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 257\n",
            "TrainLoss: 293663.031 TrainAcc: 30.150\n",
            "TestLoss:6770110.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 258\n",
            "TrainLoss: 261390.492 TrainAcc: 29.585\n",
            "TestLoss:6776830.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 259\n",
            "TrainLoss: 257329.820 TrainAcc: 29.287\n",
            "TestLoss:6781941.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 260\n",
            "TrainLoss: 281212.859 TrainAcc: 32.601\n",
            "TestLoss:6786741.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 261\n",
            "TrainLoss: 254327.258 TrainAcc: 28.722\n",
            "TestLoss:6793608.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 262\n",
            "TrainLoss: 289943.086 TrainAcc: 29.706\n",
            "TestLoss:6799940.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 263\n",
            "TrainLoss: 279264.703 TrainAcc: 30.234\n",
            "TestLoss:6802955.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 264\n",
            "TrainLoss: 348782.391 TrainAcc: 34.419\n",
            "TestLoss:6808295.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 265\n",
            "TrainLoss: 224944.250 TrainAcc: 31.661\n",
            "TestLoss:6812775.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 266\n",
            "TrainLoss: 255845.164 TrainAcc: 32.971\n",
            "TestLoss:6816428.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 267\n",
            "TrainLoss: 227610.508 TrainAcc: 27.928\n",
            "TestLoss:6817341.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 268\n",
            "TrainLoss: 229513.703 TrainAcc: 27.138\n",
            "TestLoss:6819619.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 269\n",
            "TrainLoss: 276616.203 TrainAcc: 32.856\n",
            "TestLoss:6820969.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 270\n",
            "TrainLoss: 257310.484 TrainAcc: 35.597\n",
            "TestLoss:6822535.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 271\n",
            "TrainLoss: 224345.453 TrainAcc: 30.036\n",
            "TestLoss:6823172.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 272\n",
            "TrainLoss: 217504.625 TrainAcc: 30.221\n",
            "TestLoss:6824772.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 273\n",
            "TrainLoss: 288321.008 TrainAcc: 31.056\n",
            "TestLoss:6827890.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 274\n",
            "TrainLoss: 271744.266 TrainAcc: 30.858\n",
            "TestLoss:6832934.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 275\n",
            "TrainLoss: 262299.109 TrainAcc: 33.709\n",
            "TestLoss:6836338.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 276\n",
            "TrainLoss: 326245.641 TrainAcc: 33.903\n",
            "TestLoss:6835606.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 277\n",
            "TrainLoss: 213101.258 TrainAcc: 25.654\n",
            "TestLoss:6831622.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 278\n",
            "TrainLoss: 244019.203 TrainAcc: 33.137\n",
            "TestLoss:6829225.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 279\n",
            "TrainLoss: 226719.719 TrainAcc: 25.770\n",
            "TestLoss:6829767.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 280\n",
            "TrainLoss: 216988.609 TrainAcc: 28.268\n",
            "TestLoss:6832516.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 281\n",
            "TrainLoss: 229490.453 TrainAcc: 29.711\n",
            "TestLoss:6834478.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 282\n",
            "TrainLoss: 244318.469 TrainAcc: 29.643\n",
            "TestLoss:6834573.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 283\n",
            "TrainLoss: 246562.734 TrainAcc: 30.810\n",
            "TestLoss:6833912.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 284\n",
            "TrainLoss: 235430.734 TrainAcc: 28.308\n",
            "TestLoss:6832187.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 285\n",
            "TrainLoss: 243278.461 TrainAcc: 30.634\n",
            "TestLoss:6830215.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 286\n",
            "TrainLoss: 256276.008 TrainAcc: 31.680\n",
            "TestLoss:6832542.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 287\n",
            "TrainLoss: 212892.352 TrainAcc: 33.237\n",
            "TestLoss:6834711.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 288\n",
            "TrainLoss: 270707.180 TrainAcc: 29.768\n",
            "TestLoss:6838145.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 289\n",
            "TrainLoss: 256780.617 TrainAcc: 30.517\n",
            "TestLoss:6844633.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 290\n",
            "TrainLoss: 204498.859 TrainAcc: 33.036\n",
            "TestLoss:6848750.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 291\n",
            "TrainLoss: 209539.875 TrainAcc: 29.620\n",
            "TestLoss:6850473.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 292\n",
            "TrainLoss: 221634.242 TrainAcc: 28.873\n",
            "TestLoss:6847474.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 293\n",
            "TrainLoss: 213934.578 TrainAcc: 30.928\n",
            "TestLoss:6842281.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 294\n",
            "TrainLoss: 245794.477 TrainAcc: 29.497\n",
            "TestLoss:6839335.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 295\n",
            "TrainLoss: 238138.672 TrainAcc: 30.515\n",
            "TestLoss:6837125.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 296\n",
            "TrainLoss: 195868.734 TrainAcc: 29.895\n",
            "TestLoss:6835732.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 297\n",
            "TrainLoss: 226786.633 TrainAcc: 30.632\n",
            "TestLoss:6835447.000 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 298\n",
            "TrainLoss: 235267.625 TrainAcc: 28.442\n",
            "TestLoss:6835314.500 TestAcc: 8.000\n",
            "12\n",
            "\n",
            "Epoch: 299\n",
            "TrainLoss: 250047.422 TrainAcc: 32.419\n",
            "TestLoss:6834937.000 TestAcc: 8.000\n",
            "\n",
            "start training mixup 3p\n",
            "50\n",
            "\n",
            "Epoch: 0\n",
            "TrainLoss: 8.799 TrainAcc: 10.370\n",
            "TestLoss:28.487 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 1\n",
            "TrainLoss: 19.230 TrainAcc: 18.469\n",
            "TestLoss:53.470 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 2\n",
            "TrainLoss: 32.141 TrainAcc: 27.755\n",
            "TestLoss:72.933 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 3\n",
            "TrainLoss: 32.606 TrainAcc: 25.323\n",
            "TestLoss:95.238 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 4\n",
            "TrainLoss: 28.414 TrainAcc: 25.111\n",
            "TestLoss:109.584 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 5\n",
            "TrainLoss: 21.783 TrainAcc: 30.096\n",
            "TestLoss:112.378 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 6\n",
            "TrainLoss: 30.334 TrainAcc: 26.992\n",
            "TestLoss:128.679 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 7\n",
            "TrainLoss: 23.036 TrainAcc: 27.966\n",
            "TestLoss:142.142 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 8\n",
            "TrainLoss: 23.321 TrainAcc: 32.141\n",
            "TestLoss:131.192 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 9\n",
            "TrainLoss: 32.346 TrainAcc: 30.134\n",
            "TestLoss:140.395 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 10\n",
            "TrainLoss: 33.078 TrainAcc: 28.885\n",
            "TestLoss:155.364 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 11\n",
            "TrainLoss: 37.039 TrainAcc: 27.661\n",
            "TestLoss:161.633 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 12\n",
            "TrainLoss: 44.825 TrainAcc: 26.798\n",
            "TestLoss:178.606 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 13\n",
            "TrainLoss: 57.862 TrainAcc: 27.863\n",
            "TestLoss:204.529 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 14\n",
            "TrainLoss: 38.230 TrainAcc: 32.850\n",
            "TestLoss:251.634 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 15\n",
            "TrainLoss: 78.214 TrainAcc: 27.006\n",
            "TestLoss:243.270 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 16\n",
            "TrainLoss: 83.702 TrainAcc: 25.020\n",
            "TestLoss:264.808 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 17\n",
            "TrainLoss: 77.993 TrainAcc: 29.246\n",
            "TestLoss:336.994 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 18\n",
            "TrainLoss: 103.532 TrainAcc: 29.198\n",
            "TestLoss:406.506 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 19\n",
            "TrainLoss: 105.556 TrainAcc: 30.648\n",
            "TestLoss:388.743 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 20\n",
            "TrainLoss: 150.237 TrainAcc: 26.656\n",
            "TestLoss:354.721 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 21\n",
            "TrainLoss: 197.162 TrainAcc: 30.789\n",
            "TestLoss:411.536 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 22\n",
            "TrainLoss: 216.532 TrainAcc: 26.501\n",
            "TestLoss:521.811 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 23\n",
            "TrainLoss: 281.782 TrainAcc: 27.110\n",
            "TestLoss:722.468 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 24\n",
            "TrainLoss: 402.871 TrainAcc: 29.395\n",
            "TestLoss:747.088 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 25\n",
            "TrainLoss: 315.383 TrainAcc: 29.247\n",
            "TestLoss:792.708 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 26\n",
            "TrainLoss: 443.167 TrainAcc: 26.101\n",
            "TestLoss:970.758 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 27\n",
            "TrainLoss: 530.435 TrainAcc: 28.602\n",
            "TestLoss:1055.965 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 28\n",
            "TrainLoss: 628.907 TrainAcc: 28.848\n",
            "TestLoss:1325.308 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 29\n",
            "TrainLoss: 821.625 TrainAcc: 26.325\n",
            "TestLoss:1898.245 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 30\n",
            "TrainLoss: 895.004 TrainAcc: 27.431\n",
            "TestLoss:1825.108 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 31\n",
            "TrainLoss: 1288.964 TrainAcc: 29.632\n",
            "TestLoss:2170.116 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 32\n",
            "TrainLoss: 1637.296 TrainAcc: 31.260\n",
            "TestLoss:2575.354 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 33\n",
            "TrainLoss: 1970.241 TrainAcc: 31.079\n",
            "TestLoss:2914.688 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 34\n",
            "TrainLoss: 1886.037 TrainAcc: 28.908\n",
            "TestLoss:4185.817 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 35\n",
            "TrainLoss: 3989.062 TrainAcc: 29.777\n",
            "TestLoss:4954.236 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 36\n",
            "TrainLoss: 3941.861 TrainAcc: 29.694\n",
            "TestLoss:5216.430 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 37\n",
            "TrainLoss: 3989.923 TrainAcc: 28.129\n",
            "TestLoss:6707.855 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 38\n",
            "TrainLoss: 4531.110 TrainAcc: 26.393\n",
            "TestLoss:9520.840 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 39\n",
            "TrainLoss: 6376.110 TrainAcc: 27.215\n",
            "TestLoss:11753.322 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 40\n",
            "TrainLoss: 8082.921 TrainAcc: 29.362\n",
            "TestLoss:16500.822 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 41\n",
            "TrainLoss: 6574.750 TrainAcc: 29.589\n",
            "TestLoss:18989.926 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 42\n",
            "TrainLoss: 13301.368 TrainAcc: 28.794\n",
            "TestLoss:20690.766 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 43\n",
            "TrainLoss: 10392.603 TrainAcc: 27.261\n",
            "TestLoss:24557.918 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 44\n",
            "TrainLoss: 15088.336 TrainAcc: 26.070\n",
            "TestLoss:27272.268 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 45\n",
            "TrainLoss: 19930.826 TrainAcc: 26.059\n",
            "TestLoss:30910.131 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 46\n",
            "TrainLoss: 22086.323 TrainAcc: 30.331\n",
            "TestLoss:37946.816 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 47\n",
            "TrainLoss: 22892.462 TrainAcc: 27.354\n",
            "TestLoss:50496.809 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 48\n",
            "TrainLoss: 34654.686 TrainAcc: 26.543\n",
            "TestLoss:56359.852 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 49\n",
            "TrainLoss: 35684.916 TrainAcc: 29.622\n",
            "TestLoss:80020.609 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 50\n",
            "TrainLoss: 45058.350 TrainAcc: 29.707\n",
            "TestLoss:84464.891 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 51\n",
            "TrainLoss: 66327.213 TrainAcc: 26.339\n",
            "TestLoss:123826.641 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 52\n",
            "TrainLoss: 77293.199 TrainAcc: 28.073\n",
            "TestLoss:154263.609 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 53\n",
            "TrainLoss: 74976.344 TrainAcc: 27.902\n",
            "TestLoss:178623.594 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 54\n",
            "TrainLoss: 98733.895 TrainAcc: 29.589\n",
            "TestLoss:239065.703 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 55\n",
            "TrainLoss: 156773.285 TrainAcc: 28.986\n",
            "TestLoss:237329.781 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 56\n",
            "TrainLoss: 171273.637 TrainAcc: 25.788\n",
            "TestLoss:250097.234 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 57\n",
            "TrainLoss: 186918.922 TrainAcc: 33.197\n",
            "TestLoss:411829.531 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 58\n",
            "TrainLoss: 196552.906 TrainAcc: 31.686\n",
            "TestLoss:473804.250 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 59\n",
            "TrainLoss: 263562.688 TrainAcc: 32.502\n",
            "TestLoss:620438.125 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 60\n",
            "TrainLoss: 361899.812 TrainAcc: 30.309\n",
            "TestLoss:754627.375 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 61\n",
            "TrainLoss: 511912.344 TrainAcc: 27.081\n",
            "TestLoss:735423.375 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 62\n",
            "TrainLoss: 455834.328 TrainAcc: 28.317\n",
            "TestLoss:828337.250 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 63\n",
            "TrainLoss: 560194.938 TrainAcc: 29.854\n",
            "TestLoss:1165512.500 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 64\n",
            "TrainLoss: 634091.500 TrainAcc: 26.126\n",
            "TestLoss:1291390.125 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 65\n",
            "TrainLoss: 756002.438 TrainAcc: 25.668\n",
            "TestLoss:1584576.375 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 66\n",
            "TrainLoss: 932512.719 TrainAcc: 29.477\n",
            "TestLoss:1964244.750 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 67\n",
            "TrainLoss: 1117702.938 TrainAcc: 28.210\n",
            "TestLoss:2613300.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 68\n",
            "TrainLoss: 1554999.938 TrainAcc: 29.333\n",
            "TestLoss:2837803.750 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 69\n",
            "TrainLoss: 1703539.500 TrainAcc: 29.650\n",
            "TestLoss:3224055.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 70\n",
            "TrainLoss: 2133247.750 TrainAcc: 28.150\n",
            "TestLoss:4190062.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 71\n",
            "TrainLoss: 2260147.875 TrainAcc: 23.841\n",
            "TestLoss:5279499.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 72\n",
            "TrainLoss: 3099647.375 TrainAcc: 29.542\n",
            "TestLoss:5341623.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 73\n",
            "TrainLoss: 3696425.750 TrainAcc: 31.331\n",
            "TestLoss:5750119.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 74\n",
            "TrainLoss: 3795332.500 TrainAcc: 31.640\n",
            "TestLoss:8240362.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 75\n",
            "TrainLoss: 5019970.250 TrainAcc: 26.667\n",
            "TestLoss:9137148.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 76\n",
            "TrainLoss: 5615664.000 TrainAcc: 31.636\n",
            "TestLoss:11795928.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 77\n",
            "TrainLoss: 8979581.500 TrainAcc: 26.191\n",
            "TestLoss:13845226.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 78\n",
            "TrainLoss: 10796463.000 TrainAcc: 28.679\n",
            "TestLoss:18824556.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 79\n",
            "TrainLoss: 12214179.000 TrainAcc: 23.926\n",
            "TestLoss:23058276.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 80\n",
            "TrainLoss: 13354851.500 TrainAcc: 28.330\n",
            "TestLoss:26928072.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 81\n",
            "TrainLoss: 18339036.000 TrainAcc: 30.991\n",
            "TestLoss:30172770.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 82\n",
            "TrainLoss: 17348877.000 TrainAcc: 28.515\n",
            "TestLoss:44715980.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 83\n",
            "TrainLoss: 25200422.000 TrainAcc: 28.457\n",
            "TestLoss:49877012.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 84\n",
            "TrainLoss: 27558803.000 TrainAcc: 27.356\n",
            "TestLoss:52554532.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 85\n",
            "TrainLoss: 32896342.000 TrainAcc: 30.396\n",
            "TestLoss:57120348.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 86\n",
            "TrainLoss: 61237564.000 TrainAcc: 27.390\n",
            "TestLoss:60079364.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 87\n",
            "TrainLoss: 50398964.000 TrainAcc: 27.524\n",
            "TestLoss:69242528.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 88\n",
            "TrainLoss: 64867860.000 TrainAcc: 27.973\n",
            "TestLoss:95826368.000 TestAcc: 28.000\n",
            "50\n",
            "\n",
            "Epoch: 89\n",
            "TrainLoss: 58488280.000 TrainAcc: 28.516\n",
            "TestLoss:126894088.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 90\n",
            "TrainLoss: 93992448.000 TrainAcc: 25.007\n",
            "TestLoss:137320352.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 91\n",
            "TrainLoss: 122712412.000 TrainAcc: 27.753\n",
            "TestLoss:190249840.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 92\n",
            "TrainLoss: 127564860.000 TrainAcc: 28.195\n",
            "TestLoss:258893408.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 93\n",
            "TrainLoss: 151631928.000 TrainAcc: 28.324\n",
            "TestLoss:271603456.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 94\n",
            "TrainLoss: 204578256.000 TrainAcc: 28.672\n",
            "TestLoss:311024832.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 95\n",
            "TrainLoss: 254970712.000 TrainAcc: 29.561\n",
            "TestLoss:384645760.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 96\n",
            "TrainLoss: 255283024.000 TrainAcc: 31.526\n",
            "TestLoss:486673888.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 97\n",
            "TrainLoss: 308144040.000 TrainAcc: 30.702\n",
            "TestLoss:556792128.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 98\n",
            "TrainLoss: 301442464.000 TrainAcc: 28.388\n",
            "TestLoss:767228224.000 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 99\n",
            "TrainLoss: 596448064.000 TrainAcc: 27.458\n",
            "TestLoss:901915264.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 100\n",
            "TrainLoss: 576556160.000 TrainAcc: 30.931\n",
            "TestLoss:1022913088.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 101\n",
            "TrainLoss: 677773472.000 TrainAcc: 29.628\n",
            "TestLoss:1134073472.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 102\n",
            "TrainLoss: 876663680.000 TrainAcc: 27.495\n",
            "TestLoss:1475230080.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 103\n",
            "TrainLoss: 1074608736.000 TrainAcc: 33.601\n",
            "TestLoss:1834909312.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 104\n",
            "TrainLoss: 1284829024.000 TrainAcc: 27.582\n",
            "TestLoss:2391967232.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 105\n",
            "TrainLoss: 1216620224.000 TrainAcc: 26.145\n",
            "TestLoss:2816527872.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 106\n",
            "TrainLoss: 2022154688.000 TrainAcc: 28.492\n",
            "TestLoss:3521050112.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 107\n",
            "TrainLoss: 2333885568.000 TrainAcc: 25.548\n",
            "TestLoss:4211438080.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 108\n",
            "TrainLoss: 2829352576.000 TrainAcc: 27.687\n",
            "TestLoss:5502082560.000 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 109\n",
            "TrainLoss: 3073291008.000 TrainAcc: 25.368\n",
            "TestLoss:6951696896.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 110\n",
            "TrainLoss: 4134658176.000 TrainAcc: 31.751\n",
            "TestLoss:8115074048.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 111\n",
            "TrainLoss: 5297636864.000 TrainAcc: 31.579\n",
            "TestLoss:11528031232.000 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 112\n",
            "TrainLoss: 6312414720.000 TrainAcc: 27.478\n",
            "TestLoss:11602780160.000 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 113\n",
            "TrainLoss: 7397099008.000 TrainAcc: 28.004\n",
            "TestLoss:15688336384.000 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 114\n",
            "TrainLoss: 7464417024.000 TrainAcc: 30.165\n",
            "TestLoss:16328024064.000 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 115\n",
            "TrainLoss: 9886300160.000 TrainAcc: 29.948\n",
            "TestLoss:18986039296.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 116\n",
            "TrainLoss: 12217698304.000 TrainAcc: 27.984\n",
            "TestLoss:18015946752.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 117\n",
            "TrainLoss: 12725100544.000 TrainAcc: 31.033\n",
            "TestLoss:27710902272.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 118\n",
            "TrainLoss: 17565703680.000 TrainAcc: 29.367\n",
            "TestLoss:38997266432.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 119\n",
            "TrainLoss: 19840413696.000 TrainAcc: 23.561\n",
            "TestLoss:37713084416.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 120\n",
            "TrainLoss: 17789083648.000 TrainAcc: 27.238\n",
            "TestLoss:50516418560.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 121\n",
            "TrainLoss: 25455255552.000 TrainAcc: 31.677\n",
            "TestLoss:63865171968.000 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 122\n",
            "TrainLoss: 30855021568.000 TrainAcc: 30.311\n",
            "TestLoss:62689808384.000 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 123\n",
            "TrainLoss: 37936873472.000 TrainAcc: 26.836\n",
            "TestLoss:57877053440.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 124\n",
            "TrainLoss: 24923821056.000 TrainAcc: 28.303\n",
            "TestLoss:52742373376.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 125\n",
            "TrainLoss: 22220085248.000 TrainAcc: 30.211\n",
            "TestLoss:49795858432.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 126\n",
            "TrainLoss: 26190007296.000 TrainAcc: 30.852\n",
            "TestLoss:48023060480.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 127\n",
            "TrainLoss: 22695778304.000 TrainAcc: 32.175\n",
            "TestLoss:46360391680.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 128\n",
            "TrainLoss: 18898232320.000 TrainAcc: 31.552\n",
            "TestLoss:46025428992.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 129\n",
            "TrainLoss: 22311462912.000 TrainAcc: 32.827\n",
            "TestLoss:45353947136.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 130\n",
            "TrainLoss: 17380624384.000 TrainAcc: 32.741\n",
            "TestLoss:45486125056.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 131\n",
            "TrainLoss: 16560083968.000 TrainAcc: 37.541\n",
            "TestLoss:47084093440.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 132\n",
            "TrainLoss: 18013286400.000 TrainAcc: 37.034\n",
            "TestLoss:48488345600.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 133\n",
            "TrainLoss: 16385605120.000 TrainAcc: 35.951\n",
            "TestLoss:48114524160.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 134\n",
            "TrainLoss: 14068620288.000 TrainAcc: 35.984\n",
            "TestLoss:46939348992.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 135\n",
            "TrainLoss: 17370938368.000 TrainAcc: 38.920\n",
            "TestLoss:46002774016.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 136\n",
            "TrainLoss: 14659742720.000 TrainAcc: 35.632\n",
            "TestLoss:44535873536.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 137\n",
            "TrainLoss: 13806819840.000 TrainAcc: 38.858\n",
            "TestLoss:43964878848.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 138\n",
            "TrainLoss: 10769694720.000 TrainAcc: 37.145\n",
            "TestLoss:43142037504.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 139\n",
            "TrainLoss: 15668459008.000 TrainAcc: 35.559\n",
            "TestLoss:42052837376.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 140\n",
            "TrainLoss: 14827721216.000 TrainAcc: 35.156\n",
            "TestLoss:41333538816.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 141\n",
            "TrainLoss: 13409353216.000 TrainAcc: 38.558\n",
            "TestLoss:41494458368.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 142\n",
            "TrainLoss: 12953382400.000 TrainAcc: 38.894\n",
            "TestLoss:41362378752.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 143\n",
            "TrainLoss: 11013383680.000 TrainAcc: 38.074\n",
            "TestLoss:41236979712.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 144\n",
            "TrainLoss: 11944341504.000 TrainAcc: 41.570\n",
            "TestLoss:41106702336.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 145\n",
            "TrainLoss: 15962876416.000 TrainAcc: 37.969\n",
            "TestLoss:40688873472.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 146\n",
            "TrainLoss: 9278412288.000 TrainAcc: 36.128\n",
            "TestLoss:39865016320.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 147\n",
            "TrainLoss: 11101348864.000 TrainAcc: 39.067\n",
            "TestLoss:38962343936.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 148\n",
            "TrainLoss: 11023791616.000 TrainAcc: 38.692\n",
            "TestLoss:38292209664.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 149\n",
            "TrainLoss: 10157739520.000 TrainAcc: 34.800\n",
            "TestLoss:37475561472.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 150\n",
            "TrainLoss: 10323332608.000 TrainAcc: 37.745\n",
            "TestLoss:36943536128.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 151\n",
            "TrainLoss: 8806669312.000 TrainAcc: 36.950\n",
            "TestLoss:36506759168.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 152\n",
            "TrainLoss: 9993142784.000 TrainAcc: 38.362\n",
            "TestLoss:35626950656.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 153\n",
            "TrainLoss: 8588393984.000 TrainAcc: 36.132\n",
            "TestLoss:34988544000.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 154\n",
            "TrainLoss: 8073778688.000 TrainAcc: 40.238\n",
            "TestLoss:34938138624.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 155\n",
            "TrainLoss: 7516782592.000 TrainAcc: 37.721\n",
            "TestLoss:35144757248.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 156\n",
            "TrainLoss: 7949708288.000 TrainAcc: 38.480\n",
            "TestLoss:34905407488.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 157\n",
            "TrainLoss: 8310023680.000 TrainAcc: 39.083\n",
            "TestLoss:34234181632.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 158\n",
            "TrainLoss: 7229287424.000 TrainAcc: 41.982\n",
            "TestLoss:33604171776.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 159\n",
            "TrainLoss: 7639850240.000 TrainAcc: 39.351\n",
            "TestLoss:32708075520.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 160\n",
            "TrainLoss: 6789213952.000 TrainAcc: 37.740\n",
            "TestLoss:31429447680.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 161\n",
            "TrainLoss: 6382197760.000 TrainAcc: 39.696\n",
            "TestLoss:30612977664.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 162\n",
            "TrainLoss: 6738228736.000 TrainAcc: 38.390\n",
            "TestLoss:29925519360.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 163\n",
            "TrainLoss: 6007593216.000 TrainAcc: 38.911\n",
            "TestLoss:29657350144.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 164\n",
            "TrainLoss: 6328279552.000 TrainAcc: 39.987\n",
            "TestLoss:30114672640.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 165\n",
            "TrainLoss: 5799636224.000 TrainAcc: 40.962\n",
            "TestLoss:30816223232.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 166\n",
            "TrainLoss: 6912405248.000 TrainAcc: 37.634\n",
            "TestLoss:30965331968.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 167\n",
            "TrainLoss: 6438399232.000 TrainAcc: 35.595\n",
            "TestLoss:30687940608.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 168\n",
            "TrainLoss: 5495817984.000 TrainAcc: 38.008\n",
            "TestLoss:30054275072.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 169\n",
            "TrainLoss: 4745162496.000 TrainAcc: 39.479\n",
            "TestLoss:29384198144.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 170\n",
            "TrainLoss: 6322784768.000 TrainAcc: 37.286\n",
            "TestLoss:28998488064.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 171\n",
            "TrainLoss: 5669252736.000 TrainAcc: 36.616\n",
            "TestLoss:28554881024.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 172\n",
            "TrainLoss: 6359888896.000 TrainAcc: 38.332\n",
            "TestLoss:28024430592.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 173\n",
            "TrainLoss: 5036101120.000 TrainAcc: 40.288\n",
            "TestLoss:27716085760.000 TestAcc: 22.000\n",
            "50\n",
            "\n",
            "Epoch: 174\n",
            "TrainLoss: 4952483840.000 TrainAcc: 35.881\n",
            "TestLoss:27837616128.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 175\n",
            "TrainLoss: 5122551296.000 TrainAcc: 39.282\n",
            "TestLoss:28041046016.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 176\n",
            "TrainLoss: 5079131392.000 TrainAcc: 38.898\n",
            "TestLoss:28322437120.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 177\n",
            "TrainLoss: 4713634944.000 TrainAcc: 42.307\n",
            "TestLoss:28433076224.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 178\n",
            "TrainLoss: 6165248000.000 TrainAcc: 36.162\n",
            "TestLoss:28442400768.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 179\n",
            "TrainLoss: 5590367488.000 TrainAcc: 37.324\n",
            "TestLoss:28344969216.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 180\n",
            "TrainLoss: 5227430400.000 TrainAcc: 41.063\n",
            "TestLoss:28375283712.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 181\n",
            "TrainLoss: 4773526016.000 TrainAcc: 40.316\n",
            "TestLoss:28550160384.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 182\n",
            "TrainLoss: 5219159296.000 TrainAcc: 38.183\n",
            "TestLoss:28511522816.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 183\n",
            "TrainLoss: 6521607168.000 TrainAcc: 43.361\n",
            "TestLoss:28292255744.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 184\n",
            "TrainLoss: 4793753856.000 TrainAcc: 38.891\n",
            "TestLoss:28231999488.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 185\n",
            "TrainLoss: 5136492032.000 TrainAcc: 40.016\n",
            "TestLoss:27991814144.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 186\n",
            "TrainLoss: 5842834944.000 TrainAcc: 42.269\n",
            "TestLoss:27309219840.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 187\n",
            "TrainLoss: 5194922496.000 TrainAcc: 38.973\n",
            "TestLoss:26452975616.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 188\n",
            "TrainLoss: 4753906432.000 TrainAcc: 37.149\n",
            "TestLoss:26245320704.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 189\n",
            "TrainLoss: 4278589312.000 TrainAcc: 40.044\n",
            "TestLoss:26304696320.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 190\n",
            "TrainLoss: 4681433600.000 TrainAcc: 38.621\n",
            "TestLoss:26597591040.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 191\n",
            "TrainLoss: 3975818240.000 TrainAcc: 38.878\n",
            "TestLoss:27216406528.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 192\n",
            "TrainLoss: 3907767424.000 TrainAcc: 39.122\n",
            "TestLoss:28181145600.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 193\n",
            "TrainLoss: 4128406272.000 TrainAcc: 37.259\n",
            "TestLoss:28828801024.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 194\n",
            "TrainLoss: 5433960064.000 TrainAcc: 38.131\n",
            "TestLoss:29371408384.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 195\n",
            "TrainLoss: 5199788032.000 TrainAcc: 36.438\n",
            "TestLoss:29662625792.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 196\n",
            "TrainLoss: 3785225728.000 TrainAcc: 38.314\n",
            "TestLoss:29563103232.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 197\n",
            "TrainLoss: 3941768448.000 TrainAcc: 41.073\n",
            "TestLoss:29552259072.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 198\n",
            "TrainLoss: 3763505664.000 TrainAcc: 41.874\n",
            "TestLoss:29704787968.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 199\n",
            "TrainLoss: 3833988608.000 TrainAcc: 43.628\n",
            "TestLoss:29466826752.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 200\n",
            "TrainLoss: 3804116224.000 TrainAcc: 38.375\n",
            "TestLoss:28780828672.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 201\n",
            "TrainLoss: 4069347584.000 TrainAcc: 39.882\n",
            "TestLoss:27738427392.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 202\n",
            "TrainLoss: 3880580864.000 TrainAcc: 38.012\n",
            "TestLoss:26716835840.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 203\n",
            "TrainLoss: 3765769728.000 TrainAcc: 39.107\n",
            "TestLoss:26107195392.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 204\n",
            "TrainLoss: 3729742592.000 TrainAcc: 41.385\n",
            "TestLoss:26170193920.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 205\n",
            "TrainLoss: 4578276224.000 TrainAcc: 40.737\n",
            "TestLoss:26612572160.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 206\n",
            "TrainLoss: 3368358784.000 TrainAcc: 36.639\n",
            "TestLoss:26873962496.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 207\n",
            "TrainLoss: 3914315008.000 TrainAcc: 39.715\n",
            "TestLoss:27132375040.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 208\n",
            "TrainLoss: 4255609728.000 TrainAcc: 37.980\n",
            "TestLoss:27998117888.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 209\n",
            "TrainLoss: 3281186304.000 TrainAcc: 33.897\n",
            "TestLoss:29109178368.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 210\n",
            "TrainLoss: 2747812864.000 TrainAcc: 39.034\n",
            "TestLoss:29535305728.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 211\n",
            "TrainLoss: 3132786432.000 TrainAcc: 40.212\n",
            "TestLoss:29672491008.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 212\n",
            "TrainLoss: 3080409728.000 TrainAcc: 38.378\n",
            "TestLoss:29466073088.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 213\n",
            "TrainLoss: 3587206400.000 TrainAcc: 37.775\n",
            "TestLoss:28763318272.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 214\n",
            "TrainLoss: 2930861952.000 TrainAcc: 40.219\n",
            "TestLoss:27953858560.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 215\n",
            "TrainLoss: 3420723072.000 TrainAcc: 39.686\n",
            "TestLoss:27472252928.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 216\n",
            "TrainLoss: 2824476032.000 TrainAcc: 35.685\n",
            "TestLoss:26877403136.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 217\n",
            "TrainLoss: 3128038016.000 TrainAcc: 39.263\n",
            "TestLoss:26233065472.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 218\n",
            "TrainLoss: 3148458880.000 TrainAcc: 43.010\n",
            "TestLoss:25363605504.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 219\n",
            "TrainLoss: 3347620864.000 TrainAcc: 38.412\n",
            "TestLoss:24791834624.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 220\n",
            "TrainLoss: 3210905600.000 TrainAcc: 39.570\n",
            "TestLoss:24688803840.000 TestAcc: 22.000\n",
            "50\n",
            "\n",
            "Epoch: 221\n",
            "TrainLoss: 2661351296.000 TrainAcc: 37.443\n",
            "TestLoss:24768280576.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 222\n",
            "TrainLoss: 3548913152.000 TrainAcc: 37.644\n",
            "TestLoss:24793212928.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 223\n",
            "TrainLoss: 2817317120.000 TrainAcc: 40.210\n",
            "TestLoss:24839467008.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 224\n",
            "TrainLoss: 2440530560.000 TrainAcc: 37.855\n",
            "TestLoss:24919060480.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 225\n",
            "TrainLoss: 2857823744.000 TrainAcc: 36.406\n",
            "TestLoss:25015803904.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 226\n",
            "TrainLoss: 3102343808.000 TrainAcc: 38.643\n",
            "TestLoss:25123358720.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 227\n",
            "TrainLoss: 2886103296.000 TrainAcc: 36.076\n",
            "TestLoss:25208135680.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 228\n",
            "TrainLoss: 2560487424.000 TrainAcc: 39.440\n",
            "TestLoss:25248032768.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 229\n",
            "TrainLoss: 2557099392.000 TrainAcc: 38.816\n",
            "TestLoss:25267664896.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 230\n",
            "TrainLoss: 2401793344.000 TrainAcc: 39.499\n",
            "TestLoss:25272573952.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 231\n",
            "TrainLoss: 3074174080.000 TrainAcc: 39.397\n",
            "TestLoss:25267453952.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 232\n",
            "TrainLoss: 2762455808.000 TrainAcc: 40.378\n",
            "TestLoss:25242847232.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 233\n",
            "TrainLoss: 2473766784.000 TrainAcc: 40.005\n",
            "TestLoss:25207926784.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 234\n",
            "TrainLoss: 2658873856.000 TrainAcc: 39.783\n",
            "TestLoss:25172484096.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 235\n",
            "TrainLoss: 2501929984.000 TrainAcc: 41.828\n",
            "TestLoss:25143465984.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 236\n",
            "TrainLoss: 2700600448.000 TrainAcc: 41.597\n",
            "TestLoss:25105883136.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 237\n",
            "TrainLoss: 2328995200.000 TrainAcc: 37.625\n",
            "TestLoss:25058824192.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 238\n",
            "TrainLoss: 3004861952.000 TrainAcc: 40.708\n",
            "TestLoss:24996347904.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 239\n",
            "TrainLoss: 2591399168.000 TrainAcc: 39.757\n",
            "TestLoss:24948475904.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 240\n",
            "TrainLoss: 3068996736.000 TrainAcc: 36.847\n",
            "TestLoss:24925134848.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 241\n",
            "TrainLoss: 2403901568.000 TrainAcc: 39.718\n",
            "TestLoss:24921729024.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 242\n",
            "TrainLoss: 2386457728.000 TrainAcc: 40.451\n",
            "TestLoss:24913727488.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 243\n",
            "TrainLoss: 2800410368.000 TrainAcc: 39.124\n",
            "TestLoss:24909727744.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 244\n",
            "TrainLoss: 2224391680.000 TrainAcc: 39.897\n",
            "TestLoss:24899815424.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 245\n",
            "TrainLoss: 2580369920.000 TrainAcc: 41.488\n",
            "TestLoss:24881881088.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 246\n",
            "TrainLoss: 2082119296.000 TrainAcc: 41.373\n",
            "TestLoss:24855969792.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 247\n",
            "TrainLoss: 2292689152.000 TrainAcc: 37.957\n",
            "TestLoss:24808998912.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 248\n",
            "TrainLoss: 2337434240.000 TrainAcc: 39.105\n",
            "TestLoss:24744050688.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 249\n",
            "TrainLoss: 2158779200.000 TrainAcc: 38.091\n",
            "TestLoss:24681871360.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 250\n",
            "TrainLoss: 2573590656.000 TrainAcc: 36.222\n",
            "TestLoss:24626255872.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 251\n",
            "TrainLoss: 2509958272.000 TrainAcc: 40.399\n",
            "TestLoss:24548384768.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 252\n",
            "TrainLoss: 2882684032.000 TrainAcc: 44.004\n",
            "TestLoss:24480702464.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 253\n",
            "TrainLoss: 2448300928.000 TrainAcc: 37.410\n",
            "TestLoss:24430174208.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 254\n",
            "TrainLoss: 2790220032.000 TrainAcc: 39.147\n",
            "TestLoss:24415383552.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 255\n",
            "TrainLoss: 2233658944.000 TrainAcc: 42.006\n",
            "TestLoss:24408535040.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 256\n",
            "TrainLoss: 2513908992.000 TrainAcc: 38.294\n",
            "TestLoss:24418357248.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 257\n",
            "TrainLoss: 2529240448.000 TrainAcc: 39.130\n",
            "TestLoss:24484552704.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 258\n",
            "TrainLoss: 2435166208.000 TrainAcc: 36.999\n",
            "TestLoss:24555319296.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 259\n",
            "TrainLoss: 2469095296.000 TrainAcc: 41.449\n",
            "TestLoss:24615450624.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 260\n",
            "TrainLoss: 2302112256.000 TrainAcc: 39.149\n",
            "TestLoss:24663797760.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 261\n",
            "TrainLoss: 2407095936.000 TrainAcc: 38.081\n",
            "TestLoss:24722014208.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 262\n",
            "TrainLoss: 1968695552.000 TrainAcc: 39.411\n",
            "TestLoss:24775090176.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 263\n",
            "TrainLoss: 2447972544.000 TrainAcc: 43.393\n",
            "TestLoss:24800397312.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 264\n",
            "TrainLoss: 2031487744.000 TrainAcc: 41.064\n",
            "TestLoss:24801034240.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 265\n",
            "TrainLoss: 2366383232.000 TrainAcc: 37.918\n",
            "TestLoss:24785793024.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 266\n",
            "TrainLoss: 2481828480.000 TrainAcc: 38.796\n",
            "TestLoss:24761063424.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 267\n",
            "TrainLoss: 2290901632.000 TrainAcc: 40.561\n",
            "TestLoss:24733816832.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 268\n",
            "TrainLoss: 1884321664.000 TrainAcc: 40.788\n",
            "TestLoss:24700864512.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 269\n",
            "TrainLoss: 1882648064.000 TrainAcc: 37.803\n",
            "TestLoss:24670728192.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 270\n",
            "TrainLoss: 2631774592.000 TrainAcc: 39.914\n",
            "TestLoss:24630992896.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 271\n",
            "TrainLoss: 1950630912.000 TrainAcc: 40.324\n",
            "TestLoss:24580175872.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 272\n",
            "TrainLoss: 2473580672.000 TrainAcc: 41.022\n",
            "TestLoss:24522119168.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 273\n",
            "TrainLoss: 2190468480.000 TrainAcc: 39.347\n",
            "TestLoss:24464707584.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 274\n",
            "TrainLoss: 2477993216.000 TrainAcc: 39.938\n",
            "TestLoss:24409563136.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 275\n",
            "TrainLoss: 2286666112.000 TrainAcc: 40.091\n",
            "TestLoss:24348956672.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 276\n",
            "TrainLoss: 2220289792.000 TrainAcc: 43.406\n",
            "TestLoss:24277868544.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 277\n",
            "TrainLoss: 2095885568.000 TrainAcc: 39.202\n",
            "TestLoss:24229658624.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 278\n",
            "TrainLoss: 2301437824.000 TrainAcc: 40.284\n",
            "TestLoss:24166060032.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 279\n",
            "TrainLoss: 2222205824.000 TrainAcc: 41.332\n",
            "TestLoss:24075145216.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 280\n",
            "TrainLoss: 1631048064.000 TrainAcc: 38.729\n",
            "TestLoss:24018194432.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 281\n",
            "TrainLoss: 2296828032.000 TrainAcc: 40.268\n",
            "TestLoss:23962499072.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 282\n",
            "TrainLoss: 2240547840.000 TrainAcc: 39.324\n",
            "TestLoss:23920986112.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 283\n",
            "TrainLoss: 1718004032.000 TrainAcc: 39.952\n",
            "TestLoss:23917408256.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 284\n",
            "TrainLoss: 2184704384.000 TrainAcc: 44.933\n",
            "TestLoss:23907194880.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 285\n",
            "TrainLoss: 1928339200.000 TrainAcc: 41.886\n",
            "TestLoss:23915960320.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 286\n",
            "TrainLoss: 2169593600.000 TrainAcc: 41.421\n",
            "TestLoss:23940177920.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 287\n",
            "TrainLoss: 1900063168.000 TrainAcc: 39.120\n",
            "TestLoss:23948206080.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 288\n",
            "TrainLoss: 1919985152.000 TrainAcc: 42.542\n",
            "TestLoss:23913560064.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 289\n",
            "TrainLoss: 1932915392.000 TrainAcc: 40.122\n",
            "TestLoss:23875854336.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 290\n",
            "TrainLoss: 2180519936.000 TrainAcc: 41.968\n",
            "TestLoss:23853309952.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 291\n",
            "TrainLoss: 2091311872.000 TrainAcc: 42.605\n",
            "TestLoss:23837145088.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 292\n",
            "TrainLoss: 2112459136.000 TrainAcc: 37.668\n",
            "TestLoss:23836153856.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 293\n",
            "TrainLoss: 1898654400.000 TrainAcc: 40.211\n",
            "TestLoss:23868930048.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 294\n",
            "TrainLoss: 1895211200.000 TrainAcc: 41.956\n",
            "TestLoss:23887095808.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 295\n",
            "TrainLoss: 1840208512.000 TrainAcc: 40.667\n",
            "TestLoss:23894208512.000 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 296\n",
            "TrainLoss: 1918438528.000 TrainAcc: 39.264\n",
            "TestLoss:23908620288.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 297\n",
            "TrainLoss: 2067219008.000 TrainAcc: 37.607\n",
            "TestLoss:23944118272.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 298\n",
            "TrainLoss: 1890062656.000 TrainAcc: 40.255\n",
            "TestLoss:23985199104.000 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 299\n",
            "TrainLoss: 2455812864.000 TrainAcc: 38.501\n",
            "TestLoss:24036323328.000 TestAcc: 20.000\n",
            "\n",
            "start training mixup 4p\n",
            "50\n",
            "\n",
            "Epoch: 0\n",
            "TrainLoss: 7.809 TrainAcc: 8.005\n",
            "TestLoss:23.812 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 1\n",
            "TrainLoss: 14.194 TrainAcc: 16.172\n",
            "TestLoss:33.897 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 2\n",
            "TrainLoss: 18.099 TrainAcc: 19.686\n",
            "TestLoss:48.755 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 3\n",
            "TrainLoss: 17.022 TrainAcc: 19.437\n",
            "TestLoss:41.032 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 4\n",
            "TrainLoss: 17.965 TrainAcc: 24.082\n",
            "TestLoss:47.959 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 5\n",
            "TrainLoss: 14.439 TrainAcc: 29.023\n",
            "TestLoss:50.799 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 6\n",
            "TrainLoss: 15.453 TrainAcc: 25.770\n",
            "TestLoss:49.601 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 7\n",
            "TrainLoss: 14.437 TrainAcc: 25.985\n",
            "TestLoss:64.789 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 8\n",
            "TrainLoss: 18.031 TrainAcc: 25.983\n",
            "TestLoss:71.761 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 9\n",
            "TrainLoss: 23.473 TrainAcc: 22.230\n",
            "TestLoss:71.360 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 10\n",
            "TrainLoss: 17.026 TrainAcc: 19.649\n",
            "TestLoss:74.115 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 11\n",
            "TrainLoss: 19.495 TrainAcc: 22.284\n",
            "TestLoss:94.945 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 12\n",
            "TrainLoss: 22.901 TrainAcc: 22.483\n",
            "TestLoss:110.819 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 13\n",
            "TrainLoss: 25.249 TrainAcc: 28.010\n",
            "TestLoss:101.203 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 14\n",
            "TrainLoss: 22.926 TrainAcc: 23.303\n",
            "TestLoss:105.983 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 15\n",
            "TrainLoss: 24.912 TrainAcc: 23.410\n",
            "TestLoss:120.781 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 16\n",
            "TrainLoss: 32.237 TrainAcc: 25.501\n",
            "TestLoss:126.630 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 17\n",
            "TrainLoss: 36.973 TrainAcc: 24.128\n",
            "TestLoss:130.710 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 18\n",
            "TrainLoss: 39.316 TrainAcc: 25.809\n",
            "TestLoss:137.163 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 19\n",
            "TrainLoss: 32.697 TrainAcc: 23.819\n",
            "TestLoss:167.946 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 20\n",
            "TrainLoss: 44.332 TrainAcc: 24.454\n",
            "TestLoss:183.739 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 21\n",
            "TrainLoss: 56.677 TrainAcc: 25.128\n",
            "TestLoss:191.345 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 22\n",
            "TrainLoss: 48.572 TrainAcc: 23.863\n",
            "TestLoss:219.021 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 23\n",
            "TrainLoss: 69.328 TrainAcc: 24.386\n",
            "TestLoss:260.913 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 24\n",
            "TrainLoss: 87.942 TrainAcc: 24.018\n",
            "TestLoss:264.972 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 25\n",
            "TrainLoss: 81.551 TrainAcc: 23.757\n",
            "TestLoss:286.723 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 26\n",
            "TrainLoss: 83.592 TrainAcc: 21.529\n",
            "TestLoss:295.708 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 27\n",
            "TrainLoss: 90.111 TrainAcc: 21.729\n",
            "TestLoss:316.211 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 28\n",
            "TrainLoss: 106.662 TrainAcc: 21.414\n",
            "TestLoss:389.178 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 29\n",
            "TrainLoss: 122.323 TrainAcc: 22.181\n",
            "TestLoss:489.442 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 30\n",
            "TrainLoss: 108.673 TrainAcc: 22.595\n",
            "TestLoss:525.985 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 31\n",
            "TrainLoss: 131.161 TrainAcc: 20.977\n",
            "TestLoss:503.020 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 32\n",
            "TrainLoss: 136.272 TrainAcc: 26.792\n",
            "TestLoss:484.730 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 33\n",
            "TrainLoss: 142.518 TrainAcc: 24.763\n",
            "TestLoss:546.987 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 34\n",
            "TrainLoss: 201.732 TrainAcc: 23.025\n",
            "TestLoss:625.273 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 35\n",
            "TrainLoss: 210.766 TrainAcc: 23.115\n",
            "TestLoss:783.465 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 36\n",
            "TrainLoss: 242.898 TrainAcc: 21.544\n",
            "TestLoss:740.765 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 37\n",
            "TrainLoss: 247.377 TrainAcc: 22.958\n",
            "TestLoss:830.179 TestAcc: 20.000\n",
            "50\n",
            "\n",
            "Epoch: 38\n",
            "TrainLoss: 316.773 TrainAcc: 24.566\n",
            "TestLoss:862.478 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 39\n",
            "TrainLoss: 304.038 TrainAcc: 22.935\n",
            "TestLoss:975.586 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 40\n",
            "TrainLoss: 367.460 TrainAcc: 23.131\n",
            "TestLoss:1007.891 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 41\n",
            "TrainLoss: 535.294 TrainAcc: 21.425\n",
            "TestLoss:1256.137 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 42\n",
            "TrainLoss: 406.867 TrainAcc: 23.616\n",
            "TestLoss:1595.225 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 43\n",
            "TrainLoss: 587.612 TrainAcc: 24.342\n",
            "TestLoss:1738.998 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 44\n",
            "TrainLoss: 513.708 TrainAcc: 25.755\n",
            "TestLoss:1904.257 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 45\n",
            "TrainLoss: 786.102 TrainAcc: 26.295\n",
            "TestLoss:2410.760 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 46\n",
            "TrainLoss: 672.279 TrainAcc: 24.687\n",
            "TestLoss:2679.061 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 47\n",
            "TrainLoss: 740.418 TrainAcc: 22.598\n",
            "TestLoss:2725.065 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 48\n",
            "TrainLoss: 1182.133 TrainAcc: 22.118\n",
            "TestLoss:3027.055 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 49\n",
            "TrainLoss: 1259.301 TrainAcc: 20.231\n",
            "TestLoss:3966.881 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 50\n",
            "TrainLoss: 1447.450 TrainAcc: 22.064\n",
            "TestLoss:4701.380 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 51\n",
            "TrainLoss: 1972.056 TrainAcc: 22.022\n",
            "TestLoss:5566.527 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 52\n",
            "TrainLoss: 1779.545 TrainAcc: 23.252\n",
            "TestLoss:6629.422 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 53\n",
            "TrainLoss: 2137.888 TrainAcc: 26.544\n",
            "TestLoss:7467.139 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 54\n",
            "TrainLoss: 2745.410 TrainAcc: 23.792\n",
            "TestLoss:8410.517 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 55\n",
            "TrainLoss: 2571.888 TrainAcc: 26.558\n",
            "TestLoss:9344.541 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 56\n",
            "TrainLoss: 2904.482 TrainAcc: 25.975\n",
            "TestLoss:10444.377 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 57\n",
            "TrainLoss: 4108.257 TrainAcc: 22.366\n",
            "TestLoss:12295.237 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 58\n",
            "TrainLoss: 3593.709 TrainAcc: 22.412\n",
            "TestLoss:14133.775 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 59\n",
            "TrainLoss: 5131.765 TrainAcc: 23.639\n",
            "TestLoss:13935.335 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 60\n",
            "TrainLoss: 5734.828 TrainAcc: 22.822\n",
            "TestLoss:16500.477 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 61\n",
            "TrainLoss: 6765.617 TrainAcc: 22.257\n",
            "TestLoss:18273.025 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 62\n",
            "TrainLoss: 7134.332 TrainAcc: 21.170\n",
            "TestLoss:17357.422 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 63\n",
            "TrainLoss: 9763.222 TrainAcc: 24.085\n",
            "TestLoss:20301.457 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 64\n",
            "TrainLoss: 9002.365 TrainAcc: 25.045\n",
            "TestLoss:22891.643 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 65\n",
            "TrainLoss: 12283.537 TrainAcc: 26.303\n",
            "TestLoss:29239.984 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 66\n",
            "TrainLoss: 12082.929 TrainAcc: 21.743\n",
            "TestLoss:37448.820 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 67\n",
            "TrainLoss: 11903.704 TrainAcc: 22.433\n",
            "TestLoss:41053.879 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 68\n",
            "TrainLoss: 14497.859 TrainAcc: 22.027\n",
            "TestLoss:41247.898 TestAcc: 18.000\n",
            "50\n",
            "\n",
            "Epoch: 69\n",
            "TrainLoss: 15794.570 TrainAcc: 24.514\n",
            "TestLoss:46786.691 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 70\n",
            "TrainLoss: 20701.990 TrainAcc: 23.046\n",
            "TestLoss:52294.703 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 71\n",
            "TrainLoss: 23963.764 TrainAcc: 22.002\n",
            "TestLoss:59656.789 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 72\n",
            "TrainLoss: 26576.683 TrainAcc: 24.921\n",
            "TestLoss:60981.148 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 73\n",
            "TrainLoss: 25516.564 TrainAcc: 22.698\n",
            "TestLoss:69538.070 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 74\n",
            "TrainLoss: 32276.801 TrainAcc: 24.905\n",
            "TestLoss:82094.656 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 75\n",
            "TrainLoss: 35075.557 TrainAcc: 25.187\n",
            "TestLoss:92888.430 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 76\n",
            "TrainLoss: 38764.080 TrainAcc: 22.680\n",
            "TestLoss:107039.508 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 77\n",
            "TrainLoss: 50077.836 TrainAcc: 23.433\n",
            "TestLoss:115978.906 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 78\n",
            "TrainLoss: 55927.109 TrainAcc: 22.012\n",
            "TestLoss:153128.812 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 79\n",
            "TrainLoss: 54300.350 TrainAcc: 25.831\n",
            "TestLoss:161575.906 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 80\n",
            "TrainLoss: 51942.225 TrainAcc: 23.563\n",
            "TestLoss:192096.484 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 81\n",
            "TrainLoss: 106368.234 TrainAcc: 22.384\n",
            "TestLoss:191100.188 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 82\n",
            "TrainLoss: 62641.514 TrainAcc: 22.480\n",
            "TestLoss:216962.953 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 83\n",
            "TrainLoss: 93681.441 TrainAcc: 24.422\n",
            "TestLoss:232408.062 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 84\n",
            "TrainLoss: 83125.715 TrainAcc: 22.881\n",
            "TestLoss:272739.719 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 85\n",
            "TrainLoss: 101074.961 TrainAcc: 22.971\n",
            "TestLoss:301998.312 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 86\n",
            "TrainLoss: 101841.441 TrainAcc: 20.120\n",
            "TestLoss:353016.844 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 87\n",
            "TrainLoss: 120714.523 TrainAcc: 23.582\n",
            "TestLoss:380513.281 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 88\n",
            "TrainLoss: 141556.172 TrainAcc: 25.764\n",
            "TestLoss:391429.125 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 89\n",
            "TrainLoss: 193244.430 TrainAcc: 23.263\n",
            "TestLoss:432285.562 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 90\n",
            "TrainLoss: 149135.742 TrainAcc: 26.258\n",
            "TestLoss:483098.906 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 91\n",
            "TrainLoss: 253720.102 TrainAcc: 25.567\n",
            "TestLoss:520960.719 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 92\n",
            "TrainLoss: 215567.430 TrainAcc: 22.718\n",
            "TestLoss:575639.438 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 93\n",
            "TrainLoss: 285348.125 TrainAcc: 26.286\n",
            "TestLoss:715756.812 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 94\n",
            "TrainLoss: 380259.633 TrainAcc: 24.814\n",
            "TestLoss:856074.312 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 95\n",
            "TrainLoss: 310706.641 TrainAcc: 22.510\n",
            "TestLoss:843487.250 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 96\n",
            "TrainLoss: 348223.172 TrainAcc: 21.998\n",
            "TestLoss:896202.062 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 97\n",
            "TrainLoss: 418586.250 TrainAcc: 23.725\n",
            "TestLoss:1077047.500 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 98\n",
            "TrainLoss: 480924.047 TrainAcc: 20.933\n",
            "TestLoss:1134221.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 99\n",
            "TrainLoss: 430678.688 TrainAcc: 24.644\n",
            "TestLoss:1197308.875 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 100\n",
            "TrainLoss: 540618.250 TrainAcc: 24.720\n",
            "TestLoss:1316242.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 101\n",
            "TrainLoss: 654846.375 TrainAcc: 24.085\n",
            "TestLoss:1516229.500 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 102\n",
            "TrainLoss: 706921.625 TrainAcc: 22.644\n",
            "TestLoss:1666645.750 TestAcc: 4.000\n",
            "50\n",
            "\n",
            "Epoch: 103\n",
            "TrainLoss: 782482.438 TrainAcc: 25.801\n",
            "TestLoss:1656710.125 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 104\n",
            "TrainLoss: 762020.875 TrainAcc: 21.443\n",
            "TestLoss:1839561.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 105\n",
            "TrainLoss: 1050964.156 TrainAcc: 22.996\n",
            "TestLoss:2143580.500 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 106\n",
            "TrainLoss: 1148704.750 TrainAcc: 26.861\n",
            "TestLoss:2566764.250 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 107\n",
            "TrainLoss: 1120502.062 TrainAcc: 22.904\n",
            "TestLoss:3011382.750 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 108\n",
            "TrainLoss: 1458861.688 TrainAcc: 19.545\n",
            "TestLoss:3139108.750 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 109\n",
            "TrainLoss: 1348933.500 TrainAcc: 24.227\n",
            "TestLoss:3198692.750 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 110\n",
            "TrainLoss: 1470062.125 TrainAcc: 25.453\n",
            "TestLoss:3718609.500 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 111\n",
            "TrainLoss: 1740914.938 TrainAcc: 22.297\n",
            "TestLoss:4033339.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 112\n",
            "TrainLoss: 2463158.188 TrainAcc: 24.783\n",
            "TestLoss:4681873.500 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 113\n",
            "TrainLoss: 2021548.375 TrainAcc: 25.016\n",
            "TestLoss:5552160.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 114\n",
            "TrainLoss: 2396713.375 TrainAcc: 27.574\n",
            "TestLoss:6104604.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 115\n",
            "TrainLoss: 3287325.500 TrainAcc: 24.692\n",
            "TestLoss:6279175.500 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 116\n",
            "TrainLoss: 4094210.375 TrainAcc: 23.976\n",
            "TestLoss:7323887.500 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 117\n",
            "TrainLoss: 4142221.000 TrainAcc: 26.033\n",
            "TestLoss:8584904.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 118\n",
            "TrainLoss: 5321960.000 TrainAcc: 25.693\n",
            "TestLoss:11540215.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 119\n",
            "TrainLoss: 6871390.750 TrainAcc: 21.153\n",
            "TestLoss:12524134.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 120\n",
            "TrainLoss: 8992558.500 TrainAcc: 22.890\n",
            "TestLoss:15600869.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 121\n",
            "TrainLoss: 7667796.250 TrainAcc: 24.708\n",
            "TestLoss:18581710.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 122\n",
            "TrainLoss: 8939620.500 TrainAcc: 23.089\n",
            "TestLoss:18426572.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 123\n",
            "TrainLoss: 7514897.500 TrainAcc: 25.846\n",
            "TestLoss:17695428.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 124\n",
            "TrainLoss: 7048235.000 TrainAcc: 24.108\n",
            "TestLoss:16654348.000 TestAcc: 2.000\n",
            "50\n",
            "\n",
            "Epoch: 125\n",
            "TrainLoss: 6960967.250 TrainAcc: 24.811\n",
            "TestLoss:15933887.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 126\n",
            "TrainLoss: 7976026.750 TrainAcc: 27.947\n",
            "TestLoss:15435214.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 127\n",
            "TrainLoss: 5456584.250 TrainAcc: 26.264\n",
            "TestLoss:15122633.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 128\n",
            "TrainLoss: 4702839.625 TrainAcc: 26.596\n",
            "TestLoss:15079150.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 129\n",
            "TrainLoss: 4411923.500 TrainAcc: 29.801\n",
            "TestLoss:15007053.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 130\n",
            "TrainLoss: 4921741.750 TrainAcc: 28.360\n",
            "TestLoss:14733271.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 131\n",
            "TrainLoss: 5028837.750 TrainAcc: 28.949\n",
            "TestLoss:14412712.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 132\n",
            "TrainLoss: 5478377.000 TrainAcc: 27.889\n",
            "TestLoss:14053844.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 133\n",
            "TrainLoss: 5370145.625 TrainAcc: 27.140\n",
            "TestLoss:13849403.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 134\n",
            "TrainLoss: 4441239.750 TrainAcc: 30.236\n",
            "TestLoss:13671959.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 135\n",
            "TrainLoss: 4161727.250 TrainAcc: 30.409\n",
            "TestLoss:13473312.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 136\n",
            "TrainLoss: 3439033.250 TrainAcc: 30.307\n",
            "TestLoss:13289452.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 137\n",
            "TrainLoss: 3345675.750 TrainAcc: 29.131\n",
            "TestLoss:13093235.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 138\n",
            "TrainLoss: 4361099.375 TrainAcc: 30.709\n",
            "TestLoss:12819805.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 139\n",
            "TrainLoss: 3726787.625 TrainAcc: 29.733\n",
            "TestLoss:12569192.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 140\n",
            "TrainLoss: 3544213.625 TrainAcc: 30.528\n",
            "TestLoss:12469843.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 141\n",
            "TrainLoss: 2867269.500 TrainAcc: 32.134\n",
            "TestLoss:12420333.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 142\n",
            "TrainLoss: 2952722.750 TrainAcc: 30.749\n",
            "TestLoss:12319274.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 143\n",
            "TrainLoss: 3541952.375 TrainAcc: 31.233\n",
            "TestLoss:12253512.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 144\n",
            "TrainLoss: 2654855.375 TrainAcc: 30.919\n",
            "TestLoss:12209125.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 145\n",
            "TrainLoss: 2765949.125 TrainAcc: 29.640\n",
            "TestLoss:12219232.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 146\n",
            "TrainLoss: 2740216.500 TrainAcc: 26.245\n",
            "TestLoss:12104242.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 147\n",
            "TrainLoss: 2962850.375 TrainAcc: 30.366\n",
            "TestLoss:11851482.000 TestAcc: 16.000\n",
            "50\n",
            "\n",
            "Epoch: 148\n",
            "TrainLoss: 2641494.000 TrainAcc: 30.550\n",
            "TestLoss:11561057.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 149\n",
            "TrainLoss: 2768532.375 TrainAcc: 30.233\n",
            "TestLoss:11282350.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 150\n",
            "TrainLoss: 2027990.125 TrainAcc: 30.108\n",
            "TestLoss:11176718.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 151\n",
            "TrainLoss: 2485063.500 TrainAcc: 29.131\n",
            "TestLoss:11157665.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 152\n",
            "TrainLoss: 2222796.812 TrainAcc: 32.796\n",
            "TestLoss:11124645.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 153\n",
            "TrainLoss: 2545304.000 TrainAcc: 29.879\n",
            "TestLoss:11088965.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 154\n",
            "TrainLoss: 2306844.875 TrainAcc: 31.980\n",
            "TestLoss:10964023.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 155\n",
            "TrainLoss: 2150364.625 TrainAcc: 32.828\n",
            "TestLoss:10791561.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 156\n",
            "TrainLoss: 2147090.188 TrainAcc: 32.141\n",
            "TestLoss:10668045.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 157\n",
            "TrainLoss: 2021609.312 TrainAcc: 32.880\n",
            "TestLoss:10562682.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 158\n",
            "TrainLoss: 2264262.062 TrainAcc: 33.780\n",
            "TestLoss:10441981.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 159\n",
            "TrainLoss: 2214336.688 TrainAcc: 29.254\n",
            "TestLoss:10224363.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 160\n",
            "TrainLoss: 1917306.312 TrainAcc: 30.150\n",
            "TestLoss:10010440.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 161\n",
            "TrainLoss: 1869295.625 TrainAcc: 29.370\n",
            "TestLoss:9876708.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 162\n",
            "TrainLoss: 1965095.938 TrainAcc: 31.793\n",
            "TestLoss:9763451.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 163\n",
            "TrainLoss: 1824500.688 TrainAcc: 27.652\n",
            "TestLoss:9706448.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 164\n",
            "TrainLoss: 1973037.312 TrainAcc: 29.227\n",
            "TestLoss:9657926.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 165\n",
            "TrainLoss: 1691400.688 TrainAcc: 32.918\n",
            "TestLoss:9600484.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 166\n",
            "TrainLoss: 1834565.000 TrainAcc: 31.488\n",
            "TestLoss:9563668.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 167\n",
            "TrainLoss: 1577391.062 TrainAcc: 32.453\n",
            "TestLoss:9528442.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 168\n",
            "TrainLoss: 2057480.562 TrainAcc: 34.006\n",
            "TestLoss:9540352.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 169\n",
            "TrainLoss: 1639186.375 TrainAcc: 30.864\n",
            "TestLoss:9581106.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 170\n",
            "TrainLoss: 1706825.938 TrainAcc: 30.639\n",
            "TestLoss:9617053.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 171\n",
            "TrainLoss: 1540477.625 TrainAcc: 29.297\n",
            "TestLoss:9640465.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 172\n",
            "TrainLoss: 1526918.500 TrainAcc: 30.376\n",
            "TestLoss:9580618.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 173\n",
            "TrainLoss: 1633642.125 TrainAcc: 28.985\n",
            "TestLoss:9568856.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 174\n",
            "TrainLoss: 1400128.750 TrainAcc: 28.836\n",
            "TestLoss:9542711.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 175\n",
            "TrainLoss: 1242312.062 TrainAcc: 29.966\n",
            "TestLoss:9458403.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 176\n",
            "TrainLoss: 1490331.875 TrainAcc: 33.467\n",
            "TestLoss:9317311.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 177\n",
            "TrainLoss: 1832515.188 TrainAcc: 31.733\n",
            "TestLoss:9257053.000 TestAcc: 6.000\n",
            "50\n",
            "\n",
            "Epoch: 178\n",
            "TrainLoss: 1412256.938 TrainAcc: 29.289\n",
            "TestLoss:9240734.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 179\n",
            "TrainLoss: 1259514.688 TrainAcc: 30.504\n",
            "TestLoss:9202596.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 180\n",
            "TrainLoss: 1537810.188 TrainAcc: 31.703\n",
            "TestLoss:9102488.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 181\n",
            "TrainLoss: 1149232.875 TrainAcc: 29.792\n",
            "TestLoss:8954283.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 182\n",
            "TrainLoss: 1425735.250 TrainAcc: 30.350\n",
            "TestLoss:8829711.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 183\n",
            "TrainLoss: 1463059.688 TrainAcc: 32.170\n",
            "TestLoss:8761708.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 184\n",
            "TrainLoss: 1226840.125 TrainAcc: 29.560\n",
            "TestLoss:8686861.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 185\n",
            "TrainLoss: 1209828.000 TrainAcc: 30.116\n",
            "TestLoss:8622349.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 186\n",
            "TrainLoss: 1261619.250 TrainAcc: 30.248\n",
            "TestLoss:8533021.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 187\n",
            "TrainLoss: 1020425.875 TrainAcc: 32.555\n",
            "TestLoss:8454183.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 188\n",
            "TrainLoss: 1284280.125 TrainAcc: 30.903\n",
            "TestLoss:8449248.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 189\n",
            "TrainLoss: 1408494.062 TrainAcc: 30.429\n",
            "TestLoss:8487413.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 190\n",
            "TrainLoss: 1199789.875 TrainAcc: 28.184\n",
            "TestLoss:8466152.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 191\n",
            "TrainLoss: 1150379.750 TrainAcc: 29.906\n",
            "TestLoss:8426591.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 192\n",
            "TrainLoss: 1157806.188 TrainAcc: 29.654\n",
            "TestLoss:8369644.000 TestAcc: 8.000\n",
            "50\n",
            "\n",
            "Epoch: 193\n",
            "TrainLoss: 988727.031 TrainAcc: 29.258\n",
            "TestLoss:8350459.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 194\n",
            "TrainLoss: 972749.750 TrainAcc: 31.786\n",
            "TestLoss:8329734.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 195\n",
            "TrainLoss: 1043225.750 TrainAcc: 28.380\n",
            "TestLoss:8319076.500 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 196\n",
            "TrainLoss: 1030865.250 TrainAcc: 29.815\n",
            "TestLoss:8368966.500 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 197\n",
            "TrainLoss: 1249858.406 TrainAcc: 30.752\n",
            "TestLoss:8391555.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 198\n",
            "TrainLoss: 964456.625 TrainAcc: 31.046\n",
            "TestLoss:8417697.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 199\n",
            "TrainLoss: 1022967.656 TrainAcc: 31.625\n",
            "TestLoss:8496539.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 200\n",
            "TrainLoss: 958260.844 TrainAcc: 30.626\n",
            "TestLoss:8514889.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 201\n",
            "TrainLoss: 939321.812 TrainAcc: 31.486\n",
            "TestLoss:8450230.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 202\n",
            "TrainLoss: 948159.062 TrainAcc: 30.311\n",
            "TestLoss:8331954.500 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 203\n",
            "TrainLoss: 963297.312 TrainAcc: 27.453\n",
            "TestLoss:8232669.000 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 204\n",
            "TrainLoss: 930318.062 TrainAcc: 29.311\n",
            "TestLoss:8152769.500 TestAcc: 10.000\n",
            "50\n",
            "\n",
            "Epoch: 205\n",
            "TrainLoss: 884160.812 TrainAcc: 30.566\n",
            "TestLoss:8076112.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 206\n",
            "TrainLoss: 909984.375 TrainAcc: 33.628\n",
            "TestLoss:8038683.500 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 207\n",
            "TrainLoss: 897303.500 TrainAcc: 29.450\n",
            "TestLoss:8025678.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 208\n",
            "TrainLoss: 929583.156 TrainAcc: 30.306\n",
            "TestLoss:7958723.000 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 209\n",
            "TrainLoss: 731982.812 TrainAcc: 29.217\n",
            "TestLoss:7822846.500 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 210\n",
            "TrainLoss: 676619.844 TrainAcc: 28.320\n",
            "TestLoss:7700414.500 TestAcc: 12.000\n",
            "50\n",
            "\n",
            "Epoch: 211\n",
            "TrainLoss: 725778.781 TrainAcc: 28.744\n",
            "TestLoss:7623902.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 212\n",
            "TrainLoss: 832985.438 TrainAcc: 31.636\n",
            "TestLoss:7591345.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 213\n",
            "TrainLoss: 654418.344 TrainAcc: 28.072\n",
            "TestLoss:7587734.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 214\n",
            "TrainLoss: 799680.312 TrainAcc: 28.049\n",
            "TestLoss:7619432.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 215\n",
            "TrainLoss: 803284.750 TrainAcc: 27.230\n",
            "TestLoss:7668042.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 216\n",
            "TrainLoss: 799206.750 TrainAcc: 29.776\n",
            "TestLoss:7675177.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 217\n",
            "TrainLoss: 886858.656 TrainAcc: 27.432\n",
            "TestLoss:7676821.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 218\n",
            "TrainLoss: 749403.125 TrainAcc: 30.028\n",
            "TestLoss:7632645.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 219\n",
            "TrainLoss: 809987.438 TrainAcc: 28.204\n",
            "TestLoss:7512436.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 220\n",
            "TrainLoss: 635076.438 TrainAcc: 30.057\n",
            "TestLoss:7497805.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 221\n",
            "TrainLoss: 759741.938 TrainAcc: 30.949\n",
            "TestLoss:7558505.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 222\n",
            "TrainLoss: 691025.688 TrainAcc: 29.083\n",
            "TestLoss:7562726.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 223\n",
            "TrainLoss: 800700.812 TrainAcc: 26.932\n",
            "TestLoss:7563885.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 224\n",
            "TrainLoss: 546344.094 TrainAcc: 31.256\n",
            "TestLoss:7562534.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 225\n",
            "TrainLoss: 753961.719 TrainAcc: 28.872\n",
            "TestLoss:7561152.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 226\n",
            "TrainLoss: 673923.938 TrainAcc: 32.984\n",
            "TestLoss:7562970.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 227\n",
            "TrainLoss: 879178.281 TrainAcc: 31.966\n",
            "TestLoss:7568239.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 228\n",
            "TrainLoss: 602922.000 TrainAcc: 32.199\n",
            "TestLoss:7573887.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 229\n",
            "TrainLoss: 662974.531 TrainAcc: 30.780\n",
            "TestLoss:7576188.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 230\n",
            "TrainLoss: 658726.656 TrainAcc: 27.693\n",
            "TestLoss:7576490.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 231\n",
            "TrainLoss: 723598.406 TrainAcc: 28.015\n",
            "TestLoss:7575057.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 232\n",
            "TrainLoss: 766063.344 TrainAcc: 30.897\n",
            "TestLoss:7566969.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 233\n",
            "TrainLoss: 660797.844 TrainAcc: 31.093\n",
            "TestLoss:7552808.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 234\n",
            "TrainLoss: 671584.750 TrainAcc: 30.679\n",
            "TestLoss:7534643.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 235\n",
            "TrainLoss: 627009.406 TrainAcc: 29.672\n",
            "TestLoss:7517909.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 236\n",
            "TrainLoss: 676616.312 TrainAcc: 31.473\n",
            "TestLoss:7507750.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 237\n",
            "TrainLoss: 832264.531 TrainAcc: 27.942\n",
            "TestLoss:7498734.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 238\n",
            "TrainLoss: 643283.312 TrainAcc: 28.843\n",
            "TestLoss:7491085.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 239\n",
            "TrainLoss: 649157.031 TrainAcc: 30.915\n",
            "TestLoss:7488356.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 240\n",
            "TrainLoss: 616845.281 TrainAcc: 31.270\n",
            "TestLoss:7487968.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 241\n",
            "TrainLoss: 606865.188 TrainAcc: 28.542\n",
            "TestLoss:7490298.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 242\n",
            "TrainLoss: 770558.375 TrainAcc: 29.692\n",
            "TestLoss:7493988.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 243\n",
            "TrainLoss: 661569.781 TrainAcc: 27.993\n",
            "TestLoss:7497650.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 244\n",
            "TrainLoss: 665929.750 TrainAcc: 27.430\n",
            "TestLoss:7501858.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 245\n",
            "TrainLoss: 690726.625 TrainAcc: 29.630\n",
            "TestLoss:7504079.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 246\n",
            "TrainLoss: 572991.594 TrainAcc: 28.974\n",
            "TestLoss:7503623.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 247\n",
            "TrainLoss: 639096.312 TrainAcc: 30.220\n",
            "TestLoss:7503050.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 248\n",
            "TrainLoss: 653926.625 TrainAcc: 32.884\n",
            "TestLoss:7505970.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 249\n",
            "TrainLoss: 598385.531 TrainAcc: 30.543\n",
            "TestLoss:7513143.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 250\n",
            "TrainLoss: 598450.750 TrainAcc: 28.470\n",
            "TestLoss:7521047.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 251\n",
            "TrainLoss: 559701.062 TrainAcc: 32.384\n",
            "TestLoss:7528827.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 252\n",
            "TrainLoss: 540514.047 TrainAcc: 30.314\n",
            "TestLoss:7535799.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 253\n",
            "TrainLoss: 574255.188 TrainAcc: 30.127\n",
            "TestLoss:7540297.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 254\n",
            "TrainLoss: 629416.625 TrainAcc: 28.665\n",
            "TestLoss:7545654.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 255\n",
            "TrainLoss: 607390.531 TrainAcc: 30.805\n",
            "TestLoss:7550454.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 256\n",
            "TrainLoss: 677295.875 TrainAcc: 29.677\n",
            "TestLoss:7555978.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 257\n",
            "TrainLoss: 609446.719 TrainAcc: 26.876\n",
            "TestLoss:7561966.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 258\n",
            "TrainLoss: 565749.078 TrainAcc: 30.772\n",
            "TestLoss:7563511.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 259\n",
            "TrainLoss: 602941.531 TrainAcc: 30.807\n",
            "TestLoss:7561700.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 260\n",
            "TrainLoss: 608938.719 TrainAcc: 28.671\n",
            "TestLoss:7559821.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 261\n",
            "TrainLoss: 569544.750 TrainAcc: 28.191\n",
            "TestLoss:7556950.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 262\n",
            "TrainLoss: 569522.625 TrainAcc: 32.964\n",
            "TestLoss:7550855.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 263\n",
            "TrainLoss: 609461.344 TrainAcc: 35.312\n",
            "TestLoss:7540555.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 264\n",
            "TrainLoss: 632459.719 TrainAcc: 28.970\n",
            "TestLoss:7528779.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 265\n",
            "TrainLoss: 607168.312 TrainAcc: 33.255\n",
            "TestLoss:7521851.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 266\n",
            "TrainLoss: 643621.062 TrainAcc: 31.250\n",
            "TestLoss:7517234.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 267\n",
            "TrainLoss: 673439.344 TrainAcc: 31.070\n",
            "TestLoss:7512124.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 268\n",
            "TrainLoss: 553741.531 TrainAcc: 29.736\n",
            "TestLoss:7505474.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 269\n",
            "TrainLoss: 566369.375 TrainAcc: 29.191\n",
            "TestLoss:7502132.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 270\n",
            "TrainLoss: 594275.094 TrainAcc: 28.447\n",
            "TestLoss:7500025.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 271\n",
            "TrainLoss: 545931.688 TrainAcc: 28.951\n",
            "TestLoss:7497197.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 272\n",
            "TrainLoss: 652584.250 TrainAcc: 31.398\n",
            "TestLoss:7492672.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 273\n",
            "TrainLoss: 623478.438 TrainAcc: 32.428\n",
            "TestLoss:7484651.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 274\n",
            "TrainLoss: 553242.094 TrainAcc: 30.191\n",
            "TestLoss:7477307.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 275\n",
            "TrainLoss: 630623.125 TrainAcc: 28.829\n",
            "TestLoss:7469997.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 276\n",
            "TrainLoss: 521566.469 TrainAcc: 30.250\n",
            "TestLoss:7460726.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 277\n",
            "TrainLoss: 554191.312 TrainAcc: 31.235\n",
            "TestLoss:7451664.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 278\n",
            "TrainLoss: 590951.688 TrainAcc: 30.414\n",
            "TestLoss:7442537.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 279\n",
            "TrainLoss: 547123.625 TrainAcc: 29.435\n",
            "TestLoss:7433264.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 280\n",
            "TrainLoss: 508508.562 TrainAcc: 32.542\n",
            "TestLoss:7422657.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 281\n",
            "TrainLoss: 512208.906 TrainAcc: 30.805\n",
            "TestLoss:7415071.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 282\n",
            "TrainLoss: 619511.734 TrainAcc: 31.009\n",
            "TestLoss:7410525.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 283\n",
            "TrainLoss: 514079.734 TrainAcc: 30.443\n",
            "TestLoss:7409358.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 284\n",
            "TrainLoss: 623166.250 TrainAcc: 30.014\n",
            "TestLoss:7406080.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 285\n",
            "TrainLoss: 658735.000 TrainAcc: 30.677\n",
            "TestLoss:7398610.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 286\n",
            "TrainLoss: 549924.000 TrainAcc: 26.501\n",
            "TestLoss:7392415.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 287\n",
            "TrainLoss: 550365.938 TrainAcc: 29.327\n",
            "TestLoss:7383869.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 288\n",
            "TrainLoss: 649749.938 TrainAcc: 29.872\n",
            "TestLoss:7374786.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 289\n",
            "TrainLoss: 668092.062 TrainAcc: 29.997\n",
            "TestLoss:7366846.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 290\n",
            "TrainLoss: 561574.938 TrainAcc: 30.745\n",
            "TestLoss:7360095.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 291\n",
            "TrainLoss: 584465.125 TrainAcc: 31.604\n",
            "TestLoss:7354153.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 292\n",
            "TrainLoss: 543967.906 TrainAcc: 29.497\n",
            "TestLoss:7348676.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 293\n",
            "TrainLoss: 527790.953 TrainAcc: 29.545\n",
            "TestLoss:7347411.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 294\n",
            "TrainLoss: 566922.312 TrainAcc: 31.591\n",
            "TestLoss:7346574.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 295\n",
            "TrainLoss: 537806.391 TrainAcc: 29.735\n",
            "TestLoss:7342338.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 296\n",
            "TrainLoss: 622775.562 TrainAcc: 32.660\n",
            "TestLoss:7336030.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 297\n",
            "TrainLoss: 620269.188 TrainAcc: 28.828\n",
            "TestLoss:7327897.000 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 298\n",
            "TrainLoss: 497240.469 TrainAcc: 30.484\n",
            "TestLoss:7317553.500 TestAcc: 14.000\n",
            "50\n",
            "\n",
            "Epoch: 299\n",
            "TrainLoss: 454795.562 TrainAcc: 30.848\n",
            "TestLoss:7307089.500 TestAcc: 14.000\n",
            "\n",
            "Start training ERM\n",
            "\n",
            "Epoch: 0\n",
            "TrainLoss: 18.294 TrainAcc: 6.000\n",
            "TestLoss:70.203 TestAcc: 8.000\n",
            "\n",
            "Epoch: 1\n",
            "TrainLoss: 51.253 TrainAcc: 60.000\n",
            "TestLoss:108.108 TestAcc: 10.000\n",
            "\n",
            "Epoch: 2\n",
            "TrainLoss: 26.734 TrainAcc: 65.333\n",
            "TestLoss:125.501 TestAcc: 6.000\n",
            "\n",
            "Epoch: 3\n",
            "TrainLoss: 6.839 TrainAcc: 85.333\n",
            "TestLoss:161.897 TestAcc: 6.000\n",
            "\n",
            "Epoch: 4\n",
            "TrainLoss: 2.767 TrainAcc: 86.000\n",
            "TestLoss:187.093 TestAcc: 12.000\n",
            "\n",
            "Epoch: 5\n",
            "TrainLoss: 7.891 TrainAcc: 94.667\n",
            "TestLoss:257.071 TestAcc: 16.000\n",
            "\n",
            "Epoch: 6\n",
            "TrainLoss: 0.863 TrainAcc: 96.667\n",
            "TestLoss:339.975 TestAcc: 20.000\n",
            "\n",
            "Epoch: 7\n",
            "TrainLoss: 5.835 TrainAcc: 95.333\n",
            "TestLoss:409.229 TestAcc: 16.000\n",
            "\n",
            "Epoch: 8\n",
            "TrainLoss: 2.409 TrainAcc: 93.333\n",
            "TestLoss:478.891 TestAcc: 20.000\n",
            "\n",
            "Epoch: 9\n",
            "TrainLoss: 0.131 TrainAcc: 98.000\n",
            "TestLoss:552.626 TestAcc: 22.000\n",
            "\n",
            "Epoch: 10\n",
            "TrainLoss: 2.824 TrainAcc: 96.000\n",
            "TestLoss:670.231 TestAcc: 20.000\n",
            "\n",
            "Epoch: 11\n",
            "TrainLoss: 4.207 TrainAcc: 97.333\n",
            "TestLoss:785.279 TestAcc: 16.000\n",
            "\n",
            "Epoch: 12\n",
            "TrainLoss: 5.726 TrainAcc: 97.333\n",
            "TestLoss:990.009 TestAcc: 16.000\n",
            "\n",
            "Epoch: 13\n",
            "TrainLoss: 26.247 TrainAcc: 88.667\n",
            "TestLoss:1096.721 TestAcc: 18.000\n",
            "\n",
            "Epoch: 14\n",
            "TrainLoss: 6.535 TrainAcc: 97.333\n",
            "TestLoss:1413.064 TestAcc: 16.000\n",
            "\n",
            "Epoch: 15\n",
            "TrainLoss: 24.048 TrainAcc: 93.333\n",
            "TestLoss:1735.612 TestAcc: 16.000\n",
            "\n",
            "Epoch: 16\n",
            "TrainLoss: 166.838 TrainAcc: 94.667\n",
            "TestLoss:2495.527 TestAcc: 18.000\n",
            "\n",
            "Epoch: 17\n",
            "TrainLoss: 373.299 TrainAcc: 86.000\n",
            "TestLoss:4747.022 TestAcc: 12.000\n",
            "\n",
            "Epoch: 18\n",
            "TrainLoss: 1097.170 TrainAcc: 82.000\n",
            "TestLoss:7668.430 TestAcc: 12.000\n",
            "\n",
            "Epoch: 19\n",
            "TrainLoss: 250.955 TrainAcc: 90.000\n",
            "TestLoss:12838.401 TestAcc: 20.000\n",
            "\n",
            "Epoch: 20\n",
            "TrainLoss: 794.456 TrainAcc: 88.000\n",
            "TestLoss:17207.973 TestAcc: 14.000\n",
            "\n",
            "Epoch: 21\n",
            "TrainLoss: 599.228 TrainAcc: 89.333\n",
            "TestLoss:20614.551 TestAcc: 8.000\n",
            "\n",
            "Epoch: 22\n",
            "TrainLoss: 1445.275 TrainAcc: 90.667\n",
            "TestLoss:26574.652 TestAcc: 12.000\n",
            "\n",
            "Epoch: 23\n",
            "TrainLoss: 271.931 TrainAcc: 96.667\n",
            "TestLoss:40344.574 TestAcc: 14.000\n",
            "\n",
            "Epoch: 24\n",
            "TrainLoss: 3590.327 TrainAcc: 88.000\n",
            "TestLoss:43972.488 TestAcc: 8.000\n",
            "\n",
            "Epoch: 25\n",
            "TrainLoss: 1683.338 TrainAcc: 94.000\n",
            "TestLoss:63073.859 TestAcc: 10.000\n",
            "\n",
            "Epoch: 26\n",
            "TrainLoss: 15302.754 TrainAcc: 90.667\n",
            "TestLoss:130793.992 TestAcc: 10.000\n",
            "\n",
            "Epoch: 27\n",
            "TrainLoss: 19185.640 TrainAcc: 82.667\n",
            "TestLoss:198896.234 TestAcc: 4.000\n",
            "\n",
            "Epoch: 28\n",
            "TrainLoss: 12062.148 TrainAcc: 86.667\n",
            "TestLoss:240978.156 TestAcc: 8.000\n",
            "\n",
            "Epoch: 29\n",
            "TrainLoss: 7321.097 TrainAcc: 88.667\n",
            "TestLoss:302642.031 TestAcc: 12.000\n",
            "\n",
            "Epoch: 30\n",
            "TrainLoss: 11827.625 TrainAcc: 92.667\n",
            "TestLoss:407979.531 TestAcc: 12.000\n",
            "\n",
            "Epoch: 31\n",
            "TrainLoss: 4597.611 TrainAcc: 94.000\n",
            "TestLoss:488978.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 32\n",
            "TrainLoss: 3917.055 TrainAcc: 96.667\n",
            "TestLoss:560230.375 TestAcc: 14.000\n",
            "\n",
            "Epoch: 33\n",
            "TrainLoss: 27578.970 TrainAcc: 96.000\n",
            "TestLoss:696906.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 34\n",
            "TrainLoss: 31946.928 TrainAcc: 93.333\n",
            "TestLoss:1053052.875 TestAcc: 10.000\n",
            "\n",
            "Epoch: 35\n",
            "TrainLoss: 113780.682 TrainAcc: 90.667\n",
            "TestLoss:1239209.875 TestAcc: 6.000\n",
            "\n",
            "Epoch: 36\n",
            "TrainLoss: 8481.825 TrainAcc: 94.000\n",
            "TestLoss:1614300.625 TestAcc: 18.000\n",
            "\n",
            "Epoch: 37\n",
            "TrainLoss: 202295.680 TrainAcc: 93.333\n",
            "TestLoss:2372073.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 38\n",
            "TrainLoss: 233814.789 TrainAcc: 83.333\n",
            "TestLoss:4055719.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 39\n",
            "TrainLoss: 297298.324 TrainAcc: 85.333\n",
            "TestLoss:5589824.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 40\n",
            "TrainLoss: 15496.912 TrainAcc: 96.667\n",
            "TestLoss:7340790.500 TestAcc: 10.000\n",
            "\n",
            "Epoch: 41\n",
            "TrainLoss: 24582.799 TrainAcc: 96.000\n",
            "TestLoss:9251032.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 42\n",
            "TrainLoss: 32670.098 TrainAcc: 95.333\n",
            "TestLoss:10973275.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 43\n",
            "TrainLoss: 40408.617 TrainAcc: 93.333\n",
            "TestLoss:12536831.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 44\n",
            "TrainLoss: 62689.590 TrainAcc: 96.667\n",
            "TestLoss:14222336.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 45\n",
            "TrainLoss: 71183.707 TrainAcc: 96.667\n",
            "TestLoss:16045928.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 46\n",
            "TrainLoss: 142133.648 TrainAcc: 96.667\n",
            "TestLoss:17855918.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 47\n",
            "TrainLoss: 91845.578 TrainAcc: 98.667\n",
            "TestLoss:22375340.000 TestAcc: 2.000\n",
            "\n",
            "Epoch: 48\n",
            "TrainLoss: 156002.816 TrainAcc: 95.333\n",
            "TestLoss:27784512.000 TestAcc: 4.000\n",
            "\n",
            "Epoch: 49\n",
            "TrainLoss: 238022.836 TrainAcc: 92.667\n",
            "TestLoss:34454428.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 50\n",
            "TrainLoss: 567315.500 TrainAcc: 94.667\n",
            "TestLoss:40668896.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 51\n",
            "TrainLoss: 214568.438 TrainAcc: 95.333\n",
            "TestLoss:49311984.000 TestAcc: 4.000\n",
            "\n",
            "Epoch: 52\n",
            "TrainLoss: 1843103.484 TrainAcc: 92.000\n",
            "TestLoss:55894924.000 TestAcc: 4.000\n",
            "\n",
            "Epoch: 53\n",
            "TrainLoss: 2146806.266 TrainAcc: 95.333\n",
            "TestLoss:73571368.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 54\n",
            "TrainLoss: 3558701.438 TrainAcc: 85.333\n",
            "TestLoss:74616952.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 55\n",
            "TrainLoss: 556317.109 TrainAcc: 97.333\n",
            "TestLoss:90054704.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 56\n",
            "TrainLoss: 1492797.750 TrainAcc: 98.000\n",
            "TestLoss:109290720.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 57\n",
            "TrainLoss: 53628.375 TrainAcc: 98.667\n",
            "TestLoss:131707024.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 58\n",
            "TrainLoss: 4707396.750 TrainAcc: 93.333\n",
            "TestLoss:184837200.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 59\n",
            "TrainLoss: 4058588.250 TrainAcc: 95.333\n",
            "TestLoss:249674768.000 TestAcc: 4.000\n",
            "\n",
            "Epoch: 60\n",
            "TrainLoss: 3079658.062 TrainAcc: 94.667\n",
            "TestLoss:300828064.000 TestAcc: 4.000\n",
            "\n",
            "Epoch: 61\n",
            "TrainLoss: 6385341.750 TrainAcc: 94.667\n",
            "TestLoss:364272480.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 62\n",
            "TrainLoss: 20838651.500 TrainAcc: 92.000\n",
            "TestLoss:421954112.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 63\n",
            "TrainLoss: 16751078.625 TrainAcc: 94.667\n",
            "TestLoss:482629920.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 64\n",
            "TrainLoss: 6380710.250 TrainAcc: 92.667\n",
            "TestLoss:603936960.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 65\n",
            "TrainLoss: 17094513.000 TrainAcc: 93.333\n",
            "TestLoss:731099072.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 66\n",
            "TrainLoss: 24024534.750 TrainAcc: 95.333\n",
            "TestLoss:906819328.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 67\n",
            "TrainLoss: 6090081.000 TrainAcc: 96.667\n",
            "TestLoss:1126118400.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 68\n",
            "TrainLoss: 47228880.000 TrainAcc: 91.333\n",
            "TestLoss:1360355072.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 69\n",
            "TrainLoss: 31146811.250 TrainAcc: 96.000\n",
            "TestLoss:1665209600.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 70\n",
            "TrainLoss: 43636150.000 TrainAcc: 90.667\n",
            "TestLoss:2235120896.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 71\n",
            "TrainLoss: 51940566.000 TrainAcc: 90.667\n",
            "TestLoss:2964066048.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 72\n",
            "TrainLoss: 119424940.000 TrainAcc: 89.333\n",
            "TestLoss:3836955136.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 73\n",
            "TrainLoss: 126417620.000 TrainAcc: 93.333\n",
            "TestLoss:4829211648.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 74\n",
            "TrainLoss: 50682168.000 TrainAcc: 95.333\n",
            "TestLoss:5655036416.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 75\n",
            "TrainLoss: 81384470.000 TrainAcc: 96.667\n",
            "TestLoss:6652996608.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 76\n",
            "TrainLoss: 23175312.000 TrainAcc: 96.667\n",
            "TestLoss:7849997312.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 77\n",
            "TrainLoss: 173456624.000 TrainAcc: 94.000\n",
            "TestLoss:10601810944.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 78\n",
            "TrainLoss: 1128351232.000 TrainAcc: 90.667\n",
            "TestLoss:13138801664.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 79\n",
            "TrainLoss: 583242880.000 TrainAcc: 91.333\n",
            "TestLoss:21583708160.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 80\n",
            "TrainLoss: 2449371264.000 TrainAcc: 85.333\n",
            "TestLoss:31347814400.000 TestAcc: 18.000\n",
            "\n",
            "Epoch: 81\n",
            "TrainLoss: 2458897600.000 TrainAcc: 88.000\n",
            "TestLoss:39839002624.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 82\n",
            "TrainLoss: 1794963184.000 TrainAcc: 90.000\n",
            "TestLoss:53917347840.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 83\n",
            "TrainLoss: 2309295936.000 TrainAcc: 92.667\n",
            "TestLoss:75534663680.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 84\n",
            "TrainLoss: 1533454320.000 TrainAcc: 94.000\n",
            "TestLoss:97205387264.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 85\n",
            "TrainLoss: 6372829184.000 TrainAcc: 90.000\n",
            "TestLoss:133098823680.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 86\n",
            "TrainLoss: 1235025408.000 TrainAcc: 94.667\n",
            "TestLoss:178493603840.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 87\n",
            "TrainLoss: 3093087168.000 TrainAcc: 92.667\n",
            "TestLoss:217321619456.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 88\n",
            "TrainLoss: 665260288.000 TrainAcc: 97.333\n",
            "TestLoss:261419220992.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 89\n",
            "TrainLoss: 849238784.000 TrainAcc: 96.667\n",
            "TestLoss:304564633600.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 90\n",
            "TrainLoss: 653490048.000 TrainAcc: 97.333\n",
            "TestLoss:340421410816.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 91\n",
            "TrainLoss: 48667136.000 TrainAcc: 98.667\n",
            "TestLoss:377597657088.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 92\n",
            "TrainLoss: 970590336.000 TrainAcc: 97.333\n",
            "TestLoss:409129222144.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 93\n",
            "TrainLoss: 2511831104.000 TrainAcc: 95.333\n",
            "TestLoss:466324652032.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 94\n",
            "TrainLoss: 12991161344.000 TrainAcc: 95.333\n",
            "TestLoss:532407517184.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 95\n",
            "TrainLoss: 3040216960.000 TrainAcc: 96.000\n",
            "TestLoss:669560995840.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 96\n",
            "TrainLoss: 21382228992.000 TrainAcc: 96.000\n",
            "TestLoss:774235029504.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 97\n",
            "TrainLoss: 13164287872.000 TrainAcc: 95.333\n",
            "TestLoss:915859046400.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 98\n",
            "TrainLoss: 3175451904.000 TrainAcc: 97.333\n",
            "TestLoss:1092671963136.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 99\n",
            "TrainLoss: 26805114880.000 TrainAcc: 91.333\n",
            "TestLoss:1243015938048.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 100\n",
            "TrainLoss: 2768017920.000 TrainAcc: 98.000\n",
            "TestLoss:1461690433536.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 101\n",
            "TrainLoss: 20225785856.000 TrainAcc: 96.000\n",
            "TestLoss:1703721566208.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 102\n",
            "TrainLoss: 14221163008.000 TrainAcc: 96.000\n",
            "TestLoss:2082515845120.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 103\n",
            "TrainLoss: 35502030336.000 TrainAcc: 94.000\n",
            "TestLoss:2802879561728.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 104\n",
            "TrainLoss: 112721534976.000 TrainAcc: 92.667\n",
            "TestLoss:4028453617664.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 105\n",
            "TrainLoss: 94055212032.000 TrainAcc: 94.000\n",
            "TestLoss:6088074723328.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 106\n",
            "TrainLoss: 351605866496.000 TrainAcc: 87.333\n",
            "TestLoss:8016599973888.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 107\n",
            "TrainLoss: 145832157184.000 TrainAcc: 91.333\n",
            "TestLoss:9575969325056.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 108\n",
            "TrainLoss: 191450259456.000 TrainAcc: 94.000\n",
            "TestLoss:10713282117632.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 109\n",
            "TrainLoss: 197859639296.000 TrainAcc: 94.000\n",
            "TestLoss:12329325428736.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 110\n",
            "TrainLoss: 174473428992.000 TrainAcc: 95.333\n",
            "TestLoss:14506986045440.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 111\n",
            "TrainLoss: 15765092352.000 TrainAcc: 98.667\n",
            "TestLoss:17755803746304.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 112\n",
            "TrainLoss: 325259182080.000 TrainAcc: 94.667\n",
            "TestLoss:19516826320896.000 TestAcc: 8.000\n",
            "\n",
            "Epoch: 113\n",
            "TrainLoss: 763769389056.000 TrainAcc: 96.000\n",
            "TestLoss:24258713485312.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 114\n",
            "TrainLoss: 459188011008.000 TrainAcc: 94.000\n",
            "TestLoss:33027847618560.000 TestAcc: 18.000\n",
            "\n",
            "Epoch: 115\n",
            "TrainLoss: 930469773312.000 TrainAcc: 94.667\n",
            "TestLoss:40046488977408.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 116\n",
            "TrainLoss: 273156374528.000 TrainAcc: 96.667\n",
            "TestLoss:43332034101248.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 117\n",
            "TrainLoss: 551348699136.000 TrainAcc: 92.667\n",
            "TestLoss:51287475355648.000 TestAcc: 12.000\n",
            "\n",
            "Epoch: 118\n",
            "TrainLoss: 275302645760.000 TrainAcc: 95.333\n",
            "TestLoss:61169435934720.000 TestAcc: 6.000\n",
            "\n",
            "Epoch: 119\n",
            "TrainLoss: 427621343232.000 TrainAcc: 94.000\n",
            "TestLoss:70234283180032.000 TestAcc: 10.000\n",
            "\n",
            "Epoch: 120\n",
            "TrainLoss: 438780690432.000 TrainAcc: 98.000\n",
            "TestLoss:80543278432256.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 121\n",
            "TrainLoss: 2265636077568.000 TrainAcc: 92.000\n",
            "TestLoss:101191610531840.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 122\n",
            "TrainLoss: 793798803456.000 TrainAcc: 96.667\n",
            "TestLoss:103210631036928.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 123\n",
            "TrainLoss: 120874139648.000 TrainAcc: 99.333\n",
            "TestLoss:105021681172480.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 124\n",
            "TrainLoss: 11704795136.000 TrainAcc: 99.333\n",
            "TestLoss:106536269512704.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 125\n",
            "TrainLoss: 14971256832.000 TrainAcc: 98.667\n",
            "TestLoss:107434706206720.000 TestAcc: 14.000\n",
            "\n",
            "Epoch: 126\n",
            "TrainLoss: 17191075840.000 TrainAcc: 99.333\n",
            "TestLoss:107803217756160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 127\n",
            "TrainLoss: 16421158912.000 TrainAcc: 99.333\n",
            "TestLoss:108113537531904.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 128\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:108502517284864.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 129\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:108816477716480.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 130\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109065778757632.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 131\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109270603399168.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 132\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109436764946432.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 133\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109571259498496.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 134\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109680202350592.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 135\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109768693776384.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 136\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109840533815296.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 137\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109898775920640.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 138\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109945919897600.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 139\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:109984213893120.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 140\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110016803635200.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 141\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110043236139008.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 142\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110064677421056.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 143\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110082075394048.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 144\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110096159866880.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 145\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110107593539584.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 146\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110116821008384.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 147\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110124303646720.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 148\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110130368610304.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 149\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110135301111808.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 150\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110139260534784.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 151\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110142481760256.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 152\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110145090617344.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 153\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110147204546560.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 154\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110148949377024.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 155\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110150316720128.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 156\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110151457570816.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 157\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110152338374656.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 158\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110153110126592.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 159\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110153680551936.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 160\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110154183868416.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 161\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110154569744384.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 162\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110154863345664.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 163\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155140169728.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 164\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155333107712.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 165\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155517657088.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 166\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155635097600.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 167\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155752538112.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 168\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155861590016.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 169\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110155928698880.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 170\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156012584960.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 171\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156029362176.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 172\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156079693824.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 173\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156104859648.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 174\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156130025472.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 175\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156138414080.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 176\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156163579904.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 177\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156171968512.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 178\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156180357120.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 179\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156205522944.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 180\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156213911552.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 181\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156197134336.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 182\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156213911552.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 183\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 184\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 185\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 186\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156239077376.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 187\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156239077376.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 188\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156239077376.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 189\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 190\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156239077376.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 191\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156247465984.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 192\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156239077376.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 193\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 194\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 195\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 196\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 197\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 198\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 199\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 200\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 201\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 202\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 203\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 204\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 205\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 206\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 207\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 208\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 209\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 210\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 211\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 212\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 213\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 214\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 215\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 216\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 217\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 218\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 219\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 220\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 221\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 222\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 223\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 224\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 225\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 226\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 227\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 228\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 229\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 230\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 231\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 232\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 233\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 234\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 235\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 236\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 237\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 238\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 239\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 240\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 241\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 242\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 243\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 244\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 245\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 246\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 247\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 248\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 249\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 250\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 251\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 252\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 253\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 254\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 255\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 256\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 257\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 258\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 259\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 260\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 261\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 262\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 263\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 264\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 265\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 266\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 267\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 268\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 269\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 270\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 271\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 272\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 273\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 274\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 275\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 276\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 277\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 278\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 279\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 280\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 281\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 282\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 283\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 284\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 285\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 286\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 287\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 288\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 289\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 290\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 291\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 292\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 293\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 294\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 295\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 296\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 297\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 298\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "\n",
            "Epoch: 299\n",
            "TrainLoss: 0.000 TrainAcc: 100.000\n",
            "TestLoss:110156222300160.000 TestAcc: 16.000\n",
            "for mixup ERM: global best acc =  22.0 best alpha =  [nan] global best epoch =  [9]\n",
            "for mixup 2p: global best acc =  26.0 best alpha =  [1.] global best epoch =  [22]\n",
            "for mixup 3p: global best acc =  28.0 best alpha =  [ 5. 50.] global best epoch =  [130  88]\n",
            "for mixup 4p: global best acc =  24.0 best alpha =  [0.1] global best epoch =  [50]\n",
            "ba4p 0.1\n",
            "loc4p 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}